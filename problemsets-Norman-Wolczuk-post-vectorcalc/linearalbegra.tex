\documentclass{problemset}
\usepackage{amsmath}

\usepackage{lipsum}
%\usepackage{showframe}
\usepackage{layout}


\usepackage[charter,cal=cmcal]{mathdesign} %different font
\usepackage{microtype}
\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[inline]{enumitem}
\usepackage{xparse}
\usepackage{ifthen}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{calc}
\usepackage[hidelinks]{hyperref}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
%%%
% Useful Linear Algebra macros
%%%
\newcommand{\ul}{$\underline{\phantom{xxx}}$}
\newcommand{\ull}{\underline{\phantom{xxx}}}
\newcommand{\xh}{{\hat {\mathbf x}}}
\newcommand{\yh}{{\hat {\mathbf y}}}
\newcommand{\zh}{{\hat {\mathbf z}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\proj}{\mathrm{proj}}
\newcommand{\Proj}{\mathrm{proj}}
\newcommand{\Perp}{\mathrm{perp}}
\renewcommand{\span}{\mathrm{span}\,}
\newcommand{\Span}{\mathrm{span}\,}
\newcommand{\Img}{\mathrm{img}\,}
\newcommand{\Null}{\mathrm{null}\,}
\newcommand{\Range}{\mathrm{range}\,}
\newcommand{\rref}{\mathrm{rref}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\Rank}{\mathrm{rank}}
\newcommand{\nnul}{\mathrm{nullity}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\chr}{\mathrm{char}}
\renewcommand{\d}{\mathrm{d}}

%\tcbuselibrary{skins}
%\usetikzlibrary{shadings}


%%%
% Set up the margins to use a fairly large area of the page
%%%
\textwidth=6in
\topmargin=-1in
\textheight=10in
\parskip=.07in
\parindent=0in

\begin{document}
\pagestyle{empty}

\begin{center}
{\huge\bf Inquiry Based Vector Calculus}\\

\vspace{.7in}
{
\it \copyright\,Jason Siefken, 2016 \\
Creative Commons By-Attribution Share-Alike\, \makebox(30,5){\includegraphics[height=1.2em]{by-sa.pdf}}
}
\end{center}

\section*{About the Document}

	This document was originally designed in the spring of 2016 to guide students
	through an ten week Linear Algebra course (Math 281-3) at
	Northwestern University.  

	A typical class day using the problem-sets:
	\begin{enumerate}
		\item {\bf Introduction by instructor.} This may involve giving a definition,
			a broader context for the day's topics, or answering questions.
		\item {\bf Students work on problems.} Students work individually or in pairs
			on the prescribed problem.  During this time the instructor moves around
			the room addressing questions that students may have and giving one-on-one
			coaching.
		\item {\bf Instructor intervention.} If most students have successfully solved the 
			problem, the instructor regroups the class by providing a concise 
			explanation so that everyone is ready to move to the next concept.  This
			is also time for the instructor to ensure that everyone has understood the
			main point of the exercise (since it is sometimes easy to do some computation
			while being oblivious to the larger context).

			If students are having trouble, the instructor can give hints to the group,
			and additional guidance to ensure the students don't get frustrated
			to the point of giving up.
		\item {\bf Repeat step 2.}
	\end{enumerate}

	Using this format, students are working (and happily so) most of the class.
	Further, they are especially primed to hear the insights of the instructor, 
	having already invested substantially into each problem.

	This problem-set is geared towards concepts instead of computation, though some problems
	focus on simple computation.

	{\bf License}  This document is licensed under the Creative Commons
	By-Attribution Share-Alike License.  That means, you are free to use,
	copy, and modify this document provided that you provide attribution
	to the previous copyright holders and you release your derivative work 
	under the same license.  Full text of the license is at \url{http://creativecommons.org/licenses/by-sa/4.0/}

	If you modify this document, you may add your name to the copyright list.  Also,
	if you think your contributions would be helpful to others, consider making a pull
	requestion, or opening an \emph{issue} at 
	\url{https://github.com/siefkenj/IBLLinearAlegbra}


\newpage

\setcounter{page}{1}
\pagestyle{fancy}
\rfoot{\footnotesize\it \copyright\,Jason Siefken, 2015--2016 \ \makebox(30,5){\includegraphics[height=1.2em]{by-sa.pdf}}}
\renewcommand{\headrulewidth}{0pt}

\section*{Sets of Vectors}
	\vspace{-2em}
	\question
		Write the following sets in set-builder notation
	\begin{parts}
			\item The subset $A\subseteq \R$ of real numbers larger than $\sqrt{2}$.
			\item The subset $B\subseteq \R^2$ of vectors whose first coordinate
			is twice the second.
	\end{parts}

	\begin{definition}[Unions \& Intersections]
		Two common set operations are \emph{unions} and \emph{intersections}.  
		Let $X$ and $Y$ be sets.

		\hfill\begin{minipage}{\dimexpr\textwidth-3cm}
		\begin{itemize}
			\item[(union)] $X\cup Y = \{a:a\in X\text{ or }a\in Y\}$.
			\item[(intersection)] $X\cap Y = \{a: a\in X\text{ and }a\in Y\}$.
		\end{itemize}
		\end{minipage}
	\end{definition}

	\question
	Let $X=\{1,2,3\}$ and $Y=\{2,3,4,5\}$ and $Z=\{4,5,6\}$.  Compute
	\begin{parts}
		\item $X\cup Y$
		\item $X\cap Y$
		\item $X\cup Y\cup Z$
		\item $X\cap Y\cap Z$
	\end{parts}

	\question
	Draw the following subsets of $\R^2$.
	\begin{parts}
		\item $V=\left\{\vec x\in\R^2:\vec x=\begin{bmatrix}0\\t\end{bmatrix}\text{ for some }t\in\R\right\}$.
		\item $H=\left\{\vec x\in\R^2:\vec x=\begin{bmatrix}t\\0\end{bmatrix}\text{ for some }t\in\R\right\}$.
		\item $J=\left\{\vec x\in\R^2:\vec x=t\begin{bmatrix}1\\1\end{bmatrix}\text{ for some }t\in\R\right\}$.
		\item $V\cup H$.
		\item $V\cap H$.
		\item Does $V\cup H=\R^2$?
	\end{parts}


	\vspace{-1em}
\section*{Linear Combinations, Span, and Linear Independence}
	\vspace{-1em}

	\begin{definition}[Linear Combination]
		A \emph{linear combination} of the vectors $\vec v_1,\vec v_2,\ldots,\vec v_n$ is
		a vector
		\[
			\vec w = \alpha_1\vec v_1+\alpha_2\vec v_2+\cdots+\alpha_n\vec v_n
		\]
		where $\alpha_1,\alpha_2,\ldots,\alpha_n$ are scalars.
	\end{definition}

	\question
	Let $\vec v_1=\begin{bmatrix}1\\1\end{bmatrix}$, $\vec v_2=\begin{bmatrix}1\\-1\end{bmatrix}$, and $\vec w=2\vec v_1+\vec v_2$.
	\begin{parts}
		\item Write the coordinates of $\vec w$.
		\item Draw a picture with $\vec w$, $\vec v_1$, and $\vec v_2$.
		\item Is $\mat{3\\3}$ a linear combination of $\vec v_1$ and $\vec v_2$?
		\item Is $\mat{0\\0}$ a linear combination of $\vec v_1$ and $\vec v_2$?
		\item Is $\mat{4\\0}$ a linear combination of $\vec v_1$ and $\vec v_2$?
		\item Can you find a vector in $\R^2$ that isn't a linear combination of
		$\vec v_1$ and $\vec v_2$?
		\item Can you find a vector in $\R^2$ that isn't a linear combination of
		$\vec v_1$?
	\end{parts}
	
	\begin{definition}[Span]
		The \emph{span} of a set of vectors $V$ is the set of
		all linear combinations of vectors in $V$.  That is,
		\[
			\Span V = \{\vec v:\vec v=\alpha_1\vec v_1+\alpha_2\vec v_2 + \cdots 
			+\alpha_n\vec v_n\text{ for some }\vec v_1,\vec v_2,\ldots,\vec v_n\in V
			\text{ and scalars }\alpha_1,\alpha_2,\ldots,\alpha_n\}.
		\]
	\end{definition}

	\question
	Let $\vec v_1=\mat{1\\1}$, $\vec v_2=\mat{1\\-1}$, and $\vec v_3=\mat{2\\2}$.
	\begin{parts}
		\item Draw $\Span\{\vec v_1\}$.
		\item Draw $\Span\{\vec v_2\}$.
		\item Describe $\Span\{\vec v_1,\vec v_2\}$.
		\item Describe $\Span\{\vec v_1,\vec v_3\}$.
		\item Describe $\Span\{\vec v_1,\vec v_2,\vec v_3\}$.
	\end{parts}

	\question
	Give an example of:
	\begin{parts}
		\item two vectors in $\R^3$ that span a plane;
		\item two vectors in $\R^3$ that span a line;
		\item four vectors in $\R^3$ that span a plane;
		\item a set of 50 vectors in $\R^3$ whose span is the line
		through the origin and the point $\mat{1\\2\\-3}$. \\
	\end{parts}

	In some sets, every vector is essential for computing a span.  In others,
	there are ``excess'' vectors.  This leads us to the concept of 
	linear independence.

	\begin{definition}[Linearly Dependent \& Independent]
		We say $\{\vec v_1,\vec v_2,\ldots,\vec v_n\}$ is
		\emph{linearly dependent} if for at least one $i$,
		\[
			\vec v_i\in\span\{\vec v_1,\vec v_2,\ldots,\vec v_{i-1},
			\vec v_{i+1},\ldots,\vec v_n\},
		\]
		and a set is \emph{linearly independent} otherwise.
	\end{definition}

	\question
		Let $\vec u=\mat{1\\0\\0}$, $\vec v=\mat{0\\1\\0}$, and $\vec w=\mat{1\\1\\0}$.
	\begin{parts}
		\item Describe $\Span\{\vec u,\vec v,\vec w\}$.
		\item Is $\{\vec u,\vec v,\vec w\}$ linearly independent?  Why or why not?
	\end{parts}

	Let $X=\{\vec u,\vec v,\vec w\}$.

	\begin{parts}[resume]
		\item Give a subset $Y\subseteq X$ so that $\Span Y=\Span X$ and $Y$ is
		linearly independent.
		\item Give a subset $Z\subseteq X$ so that $\Span Z=\Span X$ and $Z$ is
		linearly independent and $Z\neq Y$.
	\end{parts}
	
	\begin{definition}[Trivial Linear Combination]
	We say a linear combination 
	$a_1\vec v_1+a_2\vec v_2+\cdots +a_n\vec v_n$
	is \emph{trivial} if $a_1=a_2=\cdots=a_n=0$.
	\end{definition}
	
	\question
		Recall $\vec u=\mat{1\\0\\0}$, $\vec v=\mat{0\\1\\0}$, and $\vec w=\mat{1\\1\\0}$.
	\begin{parts}
		\item Consider the linearly dependent 
		set $\{\vec u,\vec v,\vec w\}$ (where $\vec u,\vec v,\vec w$
		are defined as above).  Can you write $\vec 0$
		as a non-trivial linear combination of vectors in this set?
		\item Consider the linearly independent 
		set $\{\vec u,\vec v\}$.  Can you write $\vec 0$
		as a non-trivial linear combination of vectors in this set?
	\end{parts}

	We now have an equivalent definition of linear dependence.

	\begin{definition}[Linearly Dependent \& Independent]
	$\{\vec v_1,\vec v_2,\ldots,\vec v_n\}$ is
	\emph{linearly dependent} if there is a non-trivial
	linear combination of $\vec v_1,\ldots,\vec v_n$ that
	equals the zero vector.
	\end{definition}

	\question
	\begin{parts}
		\item Explain how this new definition implies the old one.
		\item Explain how the old definition implies this new one.
	\end{parts}

	Since we have old def $\implies$ new def, and new def $\implies$ old def ($\implies$
	should be read aloud as `implies'), the two definitions
	are \emph{equivalent} (which we write as new def $\iff$ old def).


	\question
	Suppose for some unknown $\vec u, \vec v, \vec w$, and $\vec a$,
	\[
		\vec a = 3\vec u+2\vec v +\vec w\qquad \text{and}\qquad 
		\vec a = 2\vec u+\vec v -\vec w.
	\]
	\begin{parts}
		\item Could the set $\{\vec u,\vec v,\vec w\}$ be linearly
		independent?
	\end{parts}
	Suppose that
	\[
		\vec a = \vec u+6\vec r-\vec s
	\]
	is the \emph{only} way to write $\vec a$ using $\vec u,\vec r,\vec s$.
	\begin{parts}[resume]
		\item Is $\{\vec u,\vec r,\vec s\}$ linearly independent?
		\item Is $\{\vec u,\vec r\}$ linearly independent?
		\item Is $\{\vec u,\vec v,\vec w,\vec r\}$ linearly independent?
	\end{parts}

\section*{Subspaces and Bases}
	\vspace{-1em}
	\begin{definition}[Subspace]
		A \emph{subspace} $V\subseteq \R^n$ is a subset such that
		\begin{enumerate}
			\item[(i)] $\vec u,\vec v\in V$ implies $\vec u+\vec v\in V$.
			\item[(ii)] $\vec u\in V$ implies $k\vec u\in V$ for all scalars $k$.
		\end{enumerate}
	\end{definition}

	Subspaces give a mathematically precise definition of a ``flat space through the origin.''

	\question
	For each set, draw it and explain whether or not it is a subspace of $\R^2$.
	\begin{parts}
		\item $A=\{\vec x\in\R^2:\vec x=\mat{a\\0}\text{ for some }a\in\Z\}$.
		\item $B=\{\vec x\in\R^2:\vec x\neq \mat{0\\0}\}$.
		\item $C=\{\vec x\in\R^2:\vec x=\mat{0\\t}\text{ for some }t\in\R\}$.
		\item $D=\{\vec x\in\R^2:\vec x=\mat{0\\t}+\mat{1\\1}\text{ for some }t\in\R\}$.
		\item $E=\{\vec x\in\R^2:\vec x=\mat{0\\t}\text{ or }\vec x=\mat{t\\0}\text{ for some }t\in\R\}$.
		\item $F=\{\vec x\in\R^2:\vec x=t\mat{3\\1}\text{ for some }t\in\R\}$.
		\item $G=\span\left\{\mat{1\\1}\right\}$.
		\item $H=\span\{\vec u,\vec v\}$ for some unknown vectors $\vec u,\vec v\in\R^2$.
	\end{parts}

	\begin{definition}[Basis]
		A \emph{basis} for a subspace $V$ is a linearly independent set of vectors, $\mathcal B$,
		so that $\Span\mathcal B=V$.
	\end{definition}

	\question
	Let $\vec u=\mat{1\\0\\0}$, $\vec v=\mat{0\\1\\0}$, $\vec w=\mat{1\\1\\0}$, and $V=\span\{\vec u,\vec v,\vec w\}$.
	\begin{parts}
		\item Describe $V$.
		\item Is $\{\vec u,\vec v,\vec w\}$ a basis for $V$?  Why or why not?
		\item Give a basis for $V$.
		\item Give another basis for $V$.
		\item Is $\Span\{\vec u,\vec v\}$ a basis for $V$?  Why or why not?
	\end{parts}

	\begin{definition}[Dimension]
		The \emph{dimension} of a subspace $V$ is the number of elements in a basis for $V$.
	\end{definition}

	\begin{parts}[resume]
		\item What is the dimension of $V$?
	\end{parts}


	\question
	Let $\vec a=\mat{1\\2\\3}$, $\vec b=\mat{4\\5\\6}$, $\vec c=\mat{7\\8\\8}$ and 
	let $P=\span\{\vec a,\vec b\}$ and $Q=\span\{\vec b,\vec c\}$.
	\begin{parts}
		\item Give a basis for and the dimension of $P$.
		\item Give a basis for and the dimension of $Q$.
		\item Is $P\cap Q$ a subspace? If so, give a basis for it and its dimension.
		\item Is $P\cup Q$ a subspace? If so, give a basis for it and its dimension.
	\end{parts}

	\newpage


\section*{Systems of Linear Equations}
	
	\emph{Linear equations} are equations only involving variables, 
	multiplication by constants, and addition/subtraction.  \emph{Systems}
	of equations are sets of equations that share common variables.

	\question
	Consider the system
	\begin{equation}\label{eq2}
		\begin{array}{rcrl}
			x &-&y &= 2\\
			2x &+&y &= 1
		\end{array}
	\end{equation}

	\begin{parts}
		\item Draw the lines in (\ref{eq2}) on the same coordinate plane.
		\item Algebraically solve the system (\ref{eq2}).  What does this 
		solution represent on your graph?
	\end{parts}
	
	\question
	Let $L$ be the line given by $x-y=2$.
	\begin{parts}
		\item Write an equation of a line that doesn't intersect $L$.
		\item Write an equation of a line that intersects $L$ in 
		\begin{enumerate}
			\item one place.
			\item infinitely many places
			\item exactly two places
		\end{enumerate}
		or explain why no such equation exists.
		\item For each equation you came up with, solve the system algebraically.
		How can you tell algebraically how many solutions there are?
	\end{parts}

\subsection*{The Row Reduction Algorithm}

	\question
	\begin{parts}
		\item Solve the system
		\begin{equation}\label{eq3}
			\begin{array}{rcrcrl}
				x&-&y&-&2z &= -5\\
				2x&+&3y&+&z &= 5\\
				0x&+&2y&+&3z &= 8
			\end{array}
		\end{equation}
		any way you like.

		\item Use an augmented matrix to solve the system (\ref{eq3}).
	\end{parts}

	The system (\ref{eq3}) can be interpreted in two ways (and switching between these 
	interpretations when appropriate is one of the most powerful tools of Linear 
	Algebra).  We can think of solutions to (\ref{eq3})
	as the intersection of three planes, or we can interpret the solution
	as coefficients of a linear combination.

	\begin{parts}[resume]
		\item Rewrite (\ref{eq3}) as a vector equation of the form
		\[
			x\vec v_1+y\vec v_2+z\vec v_3 = \vec p
		\]
		where $x,y,z$ are interpreted as scalar quantities.

		\item If $(x,y,z)$ is a solution to (\ref{eq3}), explain how to get from the
		origin to $\vec p$ using only $\vec v_1, \vec v_2, \vec v_3$.
		\item If $(x,y,z)$ is a solution to \eqref{eq3}, is $\vec p\in\Span\{\vec v_1,\vec v_2,\vec v_3\}$?
	\end{parts}

	\question
	Consider the augmented matrix
	\[
		A=\left[\begin{array}{ccc|c}
			1 & 2 & -1 & -7\\
			0 & 2 & 3 & 9\\
			0 & 0 & 1 & 1
		\end{array}\right].
	\]
	\begin{parts}
		\item Write the system of equations corresponding to $A$.
		\item Solve the system of equations corresponding to $A$.
	\end{parts}

\subsection*{Infinite Solutions}
	\question
	Consider the system
	\begin{equation}\label{eq4}
		\begin{array}{rcrl}
			x&+&2y &= 3\\
			2x&+&4y &= 6
		\end{array}
	\end{equation}

	\begin{parts}
		\item How many solutions does (\ref{eq4}) have?
		\item Write the solutions to (\ref{eq4}) in vector form.
		\item What happens when you use an augmented matrix
		to solve (\ref{eq4})?
	\end{parts}


\subsection*{Free Variables}
	\question
	Suppose the row-reduced augmented matrix corresponding to 
	a system is
	\[
		B=\left[\begin{array}{cc|c}
			1 & 2 & 3\\
			0 & 0 & 0
		\end{array}\right].
	\]
	After reducing, we have 1 equation and 2 unknowns, so we can make
	$2-1=1$ choices when writing a solution.  Let's make the
	choice $y=t$.
	
	\begin{parts}
		\item With the added equation $y=t$, solve the
		system represented by $B$.
	\end{parts}

	\question
	Consider the system given by the augmented matrix
	\[
		C=\left[\begin{array}{ccccc|c}
			1&0&1&2&0&-1\\
			0&1&1&0&0&3\\
			0&0&0&0&1&4
		\end{array}\right].
	\]
	and call the variables in this system $x_1,x_2,
	x_3,x_4,x_5$.

	\begin{parts}
		\item Write the system of equations represented by $C$.
		\item Identify how many choices you can make when writing
		down a solution corresponding to $C$.
		\item Add one equation (of the form $x_i=t$ or $x_j=s$, etc.)
		for each choice you must make when solving the system.
		\item Write in vector form all solutions to $C$.
	\end{parts}

	\question
	\begin{parts}
		\item An unknown system $U$ is represented by an augmented
		matrix with 4 rows and 6 columns.  What is 
		the minimum number of
		free variables solutions to $U$ will have?
		\item An unknown system $V$ is represented by an augmented
		matrix with 6 rows and 4 columns.  What is 
		the minimum number of
		free variables solutions to $V$ will have?
	\end{parts}
	
	\question
	\begin{definition}[Homogeneous]
		A system is called \emph{homogeneous} if all equations equal $0$.
	\end{definition}

		Let $A$ be an unknown system of $3$ equations and $3$ variables and suppose
		 $(x,y,z)=(1,2,1)$ and
		$(x,y,z)=(-1,1,1)$ are solutions to $A$.
	\begin{parts}
		\item Can you produce another solution
		to the system?

		\item  Can you
		produce a solution to the homogeneous version of $A$ (the version of $A$ where every
		equation equals 0)?

		\item Suppose when you use an augmented matrix to solve the system $A$, you only have 
		one free variable.  Could $A$ be homogeneous?  Can you produce all solutions to the system $A$?
	\end{parts}



\section*{Rank}
	\begin{definition}[Rank]
		The \emph{rank} of the matrix $A$ is the number of leading ones in the 
		reduced row echelon form of $A$.
	\end{definition}

	\question
	\begin{parts}
		\item Determine the rank of
		\begin{enumerate*}
			\item $\mat{1&1\\2&2}$
			\item $\mat{1&2\\3&4}$
			\item $\mat{1&1&0\\0&0&1}$
			\item $\mat{3\\3\\2}$
			\item $\mat{1&0&1\\0&1&0\\0&0&1}$.
		\end{enumerate*}
	\end{parts}
	
	\question
	Consider the homogeneous system 
		\begin{equation}\label{eq4b}
			\begin{array}{llll}
				x&+2y&+z &= 0\\
				x&+2y&+3z &= 0\\
				-x&-2y&+z &= 0
			\end{array}
		\end{equation}
	and the non-augmented matrix of coefficients $A=\mat{1&2&1\\1&2&3\\-1&-2&1}$.
	\begin{parts}
		\item What is $\Rank(A)$?
		\item Give the general solution to \eqref{eq4b}.
		\item Are the column vectors of $A$ linearly independent?
		\item Give a non-homogeneous system with the same coefficients as \eqref{eq4b} that has
			\begin{enumerate}
				\item infinitely many solutions
				\item no solutions.
			\end{enumerate}
	\end{parts}

	\question
	\begin{parts}
		\item The rank of a $3\times 4$ matrix $A$ is $3$.  Are the column vectors of $A$ linearly independent?
		\item The rank of a $4\times 3$ matrix $B$ is $3$.  Are the column vectors of $B$ linearly independent?
	\end{parts}

\section*{Span Again}
	\question
	Consider the system 
		\begin{equation}\label{eq4bc}
			\begin{array}{llll}
				x&-y&-z &= 0\\
				0x&+1y&+2z &= 0\\
				3x&-3y&+3z &= 0
			\end{array}
		\end{equation}
	which has the unique solution $(x,y,z)=(0,0,0)$.
	\begin{parts}
		\item Give vectors $\vec u,\vec v,\vec w$ so that the system \eqref{eq4bc}
			corresponds to the vector equation $x\vec u+y\vec v+z\vec w = \vec 0$.
		\item Is $\vec w\in\Span\{\vec u,\vec v\}$? If so, write it as a linear combination
			of $\vec u$ and $\vec v$.
	\end{parts}

	The matrix $M$ is the non-augmented matrix corresponding to a homogeneous system of linear equations.
	$M$ also corresponds to the vector equation $x\vec a+y\vec b+z\vec c=\vec 0$.  Further, we know
	\[
		\rref(M) = \mat{1&0&1\\0&1&-2\\0&0&0}.
	\]
	\begin{parts}[resume]
		\item Give a solution to the vector equation $x\vec a+y\vec b+z\vec c=\vec 0$.
		\item Is $\vec c\in\Span\{\vec a,\vec b\}$?  If so, write it as a linear combination
			of $\vec a$ and $\vec b$.
		\item Do you have enough information to tell if $\{\vec a,\vec b\}$ is linearly independent?  Why or why not?
	\end{parts}

\subsection*{Finding Linearly Independent Subsets}
	\question
	Suppose when you use an augmented matrix to solve
	$a\vec u+b\vec v+c\vec w=\vec 0$ you have no free variables.
	
	\begin{parts}
		\item Is $\{\vec u,\vec v,\vec w\}$ linearly independent?
	\end{parts}
	
	Suppose when you use an augmented matrix to solve
	$a\vec u+b\vec v+c\vec w=\vec 0$, the second column corresponds to a 
	free variable.
	
	\begin{parts}[resume]
		\item Is $\{\vec u,\vec v,\vec w\}$ linearly independent?
		\item Is $\{\vec u,\vec w\}$ linearly independent?
		\item Is $\{\vec u,\vec v\}$ linearly independent?
	\end{parts}

	\begin{definition}[Maximal Linearly Independent Subset]
	Given a set of vectors $X$, a 
	\emph{maximal linearly independent subset} of $X$ is a linearly independent
	subset $V\subseteq X$ with the most possible vectors in it 
	(i.e., if you took any subset of $X$ with more vectors, it would be linearly
	dependent).
	\end{definition}

	\question
	\begin{parts}
		\item Give a maximal linearly independent subset, $T$, of
		$\left\{\mat{a\\b\\c}:a,b,c\in \R\right\}$.
		\item What is the size of $T$?
	\end{parts}

	\question
	Consider the vectors
	\[
		\vec v_1=\mat{1\\2\\1}
		\qquad
		\vec v_2=\mat{-1\\-1\\-1}
		\qquad
		\vec v_3=\mat{0\\1\\0}
		\qquad
		\vec v_4=\mat{-1\\2\\0}
		\qquad
		\vec v_5=\mat{1\\-1\\1}
	\]
	and the matrices
	\[
		A=\mat{1&-1&0&-1&1\\ 2&-1&1&2&-1\\1 & -1&0&0&1}
		\qquad \rref (A)
		=\mat{1&0&1&0&-2\\0&1&1&0&-3\\0&0&0&1&0}.
	\]
	(Notice that the columns of $A$ are the vectors $\vec v_1,\ldots, \vec v_5$)

	\begin{parts}
		\item Is $V=\{\vec v_1,\vec v_2,\vec v_3,\vec v_4,\vec v_5\}$ linearly
		independent?
		\item Pick a maximal linearly independent subset of $V$.
		\item Pick another (different) maximal linearly independent subset of $V$.
		\item Give a basis for $\Span(V)$.
		\item What is the dimension of $\Span(V)$?
	\end{parts}

	\newpage
\section*{Matrices}
	\question
	\[
		A=\mat{1&2\\3&1\\0&-1}
		\qquad
		B=\mat{-1&-1\\0&1\\1&-2}
		\qquad
		C=\mat{1&2&0\\-1&-1&-1}
	\]
	\begin{parts}
		\item Write the shape of the matrices $A,B,C$ (i.e., for each one,
		write the dimensions in $m\times n$ form).
		\item List \emph{all} products between the matrices $A,B,C$ that are
		defined. (Your list will be some subset of $AB,AC,BA,CA,BC,CB$.)
		\item Compute $AC$ and $CA$.
	\end{parts}

	\question
	\begin{parts}
		\item If the matrices $X$ and $Y$ are both square $n\times n$ matrices,
		does $XY=YX$?  Explain.
		\item If the matrices $X$ and $Y$ are both square $n\times n$ matrices,
		does $X+Y=Y+X$?  Explain.
	\end{parts}

	\question
	Consider the system represented by
	\[
		\mat{1&-3&0\\0&0&1\\0&0&0}\mat{x\\y\\z}=\vec b.
	\]
	\begin{parts}
		\item If $\vec b=\mat{1\\2\\3}$, is the set of solutions to this system a 
		point, line, plane, or other?
		\item If $\vec b=\mat{1\\1\\0}$, is the set of solutions to this system a 
		point, line, plane, or other?
	\end{parts}

	\question
	The entries of a matrix are specified by (row,column) pairs of integers.  If
	$a_{ij}$ is the $(i,j)$ entry of a matrix $A$, we may write $A=[a_{ij}]$.
	\begin{parts}
		\item Write the $2\times 2$ matrix $A$ with entries $a_{11} = 4$, $a_{12}=3$,
			$a_{21} = 7$ and $a_{22}=9$.
		\item Let $B=[b_{ij}]$ be the $3\times 3$ matrix where $b_{ij} = i+j$.  Write $B$.
		\item Let $C=[c_{ij}]$ be the $3\times 4$ matrix where $c_{ij} = 0$ if $i=j$ and
			$c_{ij}=1$ if $i\neq j$.
	\end{parts}

	\question
	\begin{definition}
		The \emph{transpose} of a matrix $A=[a_{ij}]$ is the matrix $A^{T}=[a_{ji}]$.
	\end{definition}
	Visually, the transpose of a matrix swaps rows and columns.

	\[
		A=\mat{1&1&2\\2&2&1}
	\]
	\begin{parts}
		\item What is the shape of $A$ and $A^T$?
		\item Write down $A^T$.
	\end{parts}

	$B$ and $D$ are $4\times 6$ matrices and $C$ is a $6\times 4$ matrix.

	\begin{parts}[resume]
		\item Does $(BC)^T=B^TC^T$? Explain.
		\item Does $(B+D)^T=B^T+D^T$? Explain.
		\item Compute $AA^T$ and $A^TA$ (where $A$ is the matrix defined earlier).
		What do you notice?
	\end{parts}

	\question
	\begin{definition}
		A matrix $X$ is called \emph{symmetric} if $X=X^T$.  
	\end{definition}
	Symmetric matrices have many useful properties,
	and have deep connections with orthogonality and eigenvectors (which we will get to later on).

	\begin{parts}
		\item Prove that if $W$ is a square matrix, then $V=W^TW+W+W^T$ is a symmetric
		matrix.
	\end{parts}

	\question
	\begin{definition}
		A \emph{zero matrix} is a matrix whose entries are all zeros.
		An \emph{identity matrix} is a square matrix whose diagonal
		entries are $1$ and non-diagonal entries are $0$.
	\end{definition}
	We write the $m\times n$ zero matrix as $0_{m\times n}$ or just $0$ if the shape
	is determined by context.  The $n\times n$ identity matrix is notated $I_{n\times n}$ or just
	$I$ if the shape is determined by context.

	Let $A=\mat{1&2&3\\4&5&6\\7&8&9}$.
	\begin{parts}
		\item Write down the $3\times 3$ identity matrix and the $3\times 3$ zero
		matrix.
		\item Compute $I_{3\times 3}A$, $AI_{3\times 3}$, $0_{3\times 3}A$,
		and $A0_{3\times 3}$.
		\item If we were to think of matrices as numbers, what numbers would the
		zero matrix and the identity matrix correspond to?
	\end{parts}

	\question
	\begin{parts}
		\item Solve the matrix equation
		\[
			I_{4\times 4}\mat{x\\y\\z\\w} = \mat{2\\3\\1\\-1}.
		\]
	\end{parts}



\newpage
\section*{Linear Transformations}
\vspace{-1.5em}
	
	\question
	$\mathcal R:\R^2\to\R^2$ is the transformation that rotates vectors counter-clockwise 
	by $90^\circ$.
	\begin{parts}
		\item Compute $\mathcal R\mat{1\\0}$ and $\mathcal R\mat{0\\1}$.
		\item Compute $\mathcal R\mat{1\\1}$.  How does this relate to
			$\mathcal R\mat{1\\0}$ and $\mathcal R\mat{0\\1}$?
		\item What is $\mathcal R\left(a\mat{1\\0}+b\mat{0\\1}\right)$?
		\item Write down a matrix $R$ so that $R\vec v$ is $\vec v$ rotated
			counter clockwise by $90^\circ$.
	\end{parts}

	\question
	$\mathcal S:\R^3\to\R^3$ stretches in the $\zh$ direction  by a factor of $2$
	and contracts in the $\yh$ direction by a factor of $3$.
	\begin{parts}
		\item Write a matrix representation of $\mathcal S$.
	\end{parts}

	\begin{definition}[Linear Transformation]
		If $V$ and $W$ are vector spaces, a function $T:V\to W$ is called a \emph{linear transformation}
		if 
		\[
			T(\vec u+\vec v)=T\vec u+T\vec v \qquad\text{and}\qquad
			T(\alpha \vec v)=\alpha T\vec v
		\]
		for vectors $\vec u,\vec v\in V$ and all scalars $\alpha$.
	\end{definition}

	\question
	\begin{parts}
		\item Classify the following as linear transformation or not
			\begin{enumerate}
				\item $\mathcal R$ from above.
				\item $\mathcal S$ from above.
				\item $W:\R^2\to\R^2$ where $W\mat{x\\y}=\mat{x^2\\y}$.
				\item $T:\R^2\to\R^2$ where $T\mat{x\\y}=\mat{x+2\\y}$.
				\item $\mathcal P:\R^2\to\R^2$ where $\mathcal P\mat{x\\y}=\proj_{\vec u}\mat{x\\y}$ and 
					$\vec u=\mat{2\\3}$.
			\end{enumerate}
	\end{parts}

	It turns out every linear transformation can be written as a matrix (in fact
	this is why matrix multiplication was invented).

	\question
	Define $\mathcal P$ to be projection onto $\vec u=\mat{2\\3}$.
	\begin{parts}
		\item Write down a matrix for $\mathcal P$.
		%\item What is the null space of $\mathcal P$?
		\item What is the rank of the matrix corresponding to $\mathcal P$?
		%\item Is $\mathcal P$ invertible?
	\end{parts}

	Matrix multiplication was designed to exactly model composition of linear transformations.
	\begin{parts}[resume]
		\item Write down a matrix for $\mathcal P$ and for $\mathcal R$, the counter-clockwise rotation
			by $90^\circ$.
		\item Write down matrices for $\mathcal P\circ\mathcal R$ and $\mathcal R\circ \mathcal P$.
	\end{parts}

	\begin{definition}[Range]
		The \emph{range} (or \emph{image}) of a linear transformation $T:V\to W$ is the set of vectors that
		$T$ can output.  That is,
		\[
			\Range(T)=\{\vec y\in W:\vec y=T\vec x\text{ for some }\vec x\in V\}.
		\]
	\end{definition}
	\begin{definition}[Null Space]
		The \emph{null space} (or \emph{kernel}) of a linear transformation $T:V\to W$ is the
		set of vectors that get mapped to zero under $T$.  That is,
		\[
			\Null(T)=\{\vec x\in V:T\vec x=\vec 0\}.
		\]
	\end{definition}

	\question
	Let $\mathcal P:\R^2\to\R^2$ be projection onto the vector $\vec u=\mat{2\\3}$ (like before).
	\begin{parts}
		\item What is the range of $\mathcal P$?
		\item What is the null space of $\mathcal P$?
	\end{parts}

	\question
	Let $T:\R^n\to\R^m$ be an arbitrary linear transformation.
	\begin{parts}
		\item Show that the null space of $T$ is a subspace.
		\item Show that the range of $T$ is a subspace.
	\end{parts}


	\begin{definition}[Fundamental Subspaces]
		Associated with any matrix $M$ are three fundamental subspaces: the \emph{row space}
		of $M$ is the span of the rows of $M$; the \emph{column space} of $M$ is the span
		of the columns of $M$; and the \emph{null space} of $M$ is the set of solutions
		to $M\vec x=\vec 0$. 
		
	\end{definition}

	\question
	Consider $A=\mat{1&0&0\\0&1&0}$.
	\begin{parts}
		\item Describe the row space of $A$.
		\item Describe the column space of $A$.
		\item Is the row space of $A$ the same as the column space of $A$?
		\item Describe the set of all vectors perpendicular to the rows of $A$.
		\item Describe the null space of $A$.
	\end{parts}

	\question
	\[
		B=\mat{1&2&3\\1&1&1}\qquad C=\rref(B)=\mat{1&0&-1\\0&1&2}
	\]
	\begin{parts}
		\item How does the row space of $B$ relate to the row space of $C$?
		\item How does the null space of $B$ relate to the null space of $C$?
		\item Compute the null space of $B$.
	\end{parts}

	\question
	\[
		P=\mat{0&0\\1&2}\qquad Q=\rref(P)=\mat{1&2\\0&0}
	\]
	\begin{parts}
		\item How does the column space of $P$ relate to the column space of $Q$?
		\item Describe the columns space of $P$ and the column space of $Q$.
	\end{parts}

	\begin{theorem}[Rank-nullity Theorem]
	The \emph{nullity} of a matrix is the dimension of the null space.

	The rank-nullity theorem states
	\[
		\rank(A)+\nnul(A) = \#\text{ of columns in }A.
	\]
	\end{theorem}

	\question
	The vectors $\vec u,\vec v\in\R^9$ are linearly independent and $\vec w=2\vec u-\vec v$.
	Define $A=[\vec u|\vec v|\vec w]$.
	\begin{parts}
		\item What is the rank and nullity of $A^T$?
		\item What is the rank and nullity of $A$?
	\end{parts}

\newpage

\section*{Matrix Inverses}

	\question
	\begin{parts}
		\item Apply the row operation $R_3\to R_3+2R_1$ to the $3\times 3$ identity
		matrix and call the result $E_1$.
		\item Apply the row operation $R_3\to R_3-2R_1$ to the $3\times 3$ identity
		matrix and call the result $E_2$.
	\end{parts}

	\begin{definition}
	An \emph{elementary matrix} is the identity matrix with a single row operation applied.
	\end{definition}

	\[
		A=\mat{1&2&3\\4&5&6\\7&8&9}
	\]
	\begin{parts}[resume]
		\item Compute $E_1A$ and $E_2A$.  How do the resulting matrices relate to row
		operations?
		\item Without computing, what should the result of applying the row
		operation $R_3\to R_3-2R_1$ to $E_1$ be?  Compute and verify.
		\item Without computing, what should $E_1E_2$ be?  What about $E_2E_1$?
		Now compute and verify.
	\end{parts}

	\begin{definition}
		The \emph{inverse} of an $n\times n$ matrix $A$ is an $n\times n$
		matrix $B$ such that $AB=I_{n\times n}=BA$.
		In this case, $B$ is called the inverse of $A$ and is notated as $A^{-1}$.
	\end{definition}

	\question
	Consider the matrices 
	\[
		A=\mat{1&2&0\\0&1&0\\-3&-6&1}\qquad
		B=\mat{1&0&0\\0&1&0}\qquad
		C=\mat{1&0\\0&1\\0&0}
	\]
	\[
		D=\mat{1&-2&0\\0&1&0\\3&0&1}\qquad
		E=\mat{1&0&0\\0&2&0\\0&1&1}\qquad
		F=\mat{1&0&0\\0&1&0\\0&0&1}
	\]
	\begin{parts}
		\item Which pairs of matrices above are inverses of each other?
	\end{parts}

	\question
	\[
		B=\mat{1 &4\\0 &2}
	\]
	\begin{parts}
		\item Use two row operations to reduce $B$ to $I_{2\times 2}$
		and write an elementary matrix $E_1$ corresponding to the first operation
		and $E_2$ corresponding to the second.
		\item What is $E_2E_1B$?
		\item Find $B^{-1}$.
		\item Can you outline a procedure for finding the inverse of a matrix
		using elementary matrices?
	\end{parts}

	\question
	\[
		A=\mat{1&2&-1\\2&2&4\\1&3&-3}\qquad
		\vec b=\mat{1\\2\\3}\qquad
		C=[A|\vec b]\qquad
		A^{-1}=\mat{9&-3/2&-5\\-5&1&3\\-2&1/2&1}
	\]
	\begin{parts}
		\item What is $A^{-1}A$?
		\item What is $\rref(A)$?
		\item What is $\rref(C)$? (Hint, there is no need to actually do row reduction!)
		\item Solve the system $A\vec x=\vec b$.
	\end{parts}

	\question
	\begin{parts}
		\item For two square matrices $X,Y$, should $(XY)^{-1}=X^{-1}Y^{-1}$?
		\item If $M$ is a matrix corresponding to a non-invertible linear transformation $T$,
			could $M$ be invertible?
	\end{parts}


\section*{Change of Basis}
	\question
	Let $\vec b_1=\mat{1\\1}$, $\vec b_2=\mat{1\\-1}$, $\vec c=\mat{4\\0}$, and $\mathcal B=\{\vec b_1,\vec b_2\}$.
	\begin{parts}
		\item Is $\mathcal B$ a basis for $\R^2$?
		\item Find coefficients $\alpha_1$ and $\alpha_2$ so that $\vec c=\alpha_1\vec b_1+\alpha_2\vec b_2$.
	\end{parts}
	We call the vector $\mat{\alpha_1\\\alpha_2}$ the representation of $\vec c$ in the $\mathcal B$ basis and notate
	this by $[\vec c]_{\mathcal B}=\mat{\alpha_1\\\alpha_2}$.
	\begin{parts}[resume]
		\item Compute $[\vec e_1]_{\mathcal B}$ and $[\vec e_2]_{\mathcal B}$.
	\end{parts}
	Let $X=[\vec b_1|\vec b_2]$ be the matrix whose columns are $\vec b_1$ and $\vec b_2$.
	\begin{parts}[resume]
		\item Compute $X[\vec c]_{\mathcal B}$.  What do you notice?
	\end{parts}

	
	\question
	Let $\mathcal S=\{\vec e_1,\vec e_2,\ldots,\vec e_n\}$ be the standard basis for $\R^n$.
	Given a basis $\mathcal B=\{\vec b_1,\vec b_2,\ldots,\vec b_n\}$ for $\R^n$, the 
	matrix $X=[\vec b_1|\vec b_2|\cdots|\vec b_n]$ converts
	vectors from the $\mathcal B$ basis into the standard basis.  In other words,
	\[
		X[\vec v]_{\mathcal B} = [\vec v]_{\mathcal S}.
	\]
	\begin{parts}
		\item Should $X^{-1}$ exist? Explain.
		\item Consider the equation\[
				X^{-1}[\vec v]_{?} = [\vec v]_{?}.
			\]
			Can you fill in the ``?'' symbols so that the equation makes sense?
		\item What is $[\vec b_1]_{\mathcal B}$?  How about $[\vec b_2]_{\mathcal B}$?  Can
			you generalize to $[\vec b_i]_{\mathcal B}$?
	\end{parts}

	\question
	Let $\vec c_1=\mat{2\\1}$, $\vec c_2=\mat{5\\3}$, $\mathcal C=\{\vec c_1,\vec c_2\}$, and $A=\mat{2&5\\1&3}$.
	Note that $A^{-1}=\mat{3&-5\\-1&2}$ and that $A$ changes vectors from the $\mathcal C$ basis to the standard
	basis and $A^{-1}$ changes vectors from the standard basis to the $\mathcal C$ basis.
	\begin{parts}
		\item Compute $[\vec c_1]_{\mathcal C}$ and $[\vec c_2]_{\mathcal C}$.
	\end{parts}
	Let $T:\R^2\to\R^2$ be the linear transformation that stretches in the $\vec c_1$ direction by a factor of $2$
	and doesn't stretch in the $\vec c_2$ direction at all.
	\begin{parts}[resume]
		\item Compute $T\mat{2\\1}$ and $T\mat{5\\3}$.
		\item Compute $[T\vec c_1]_{\mathcal C}$ and $[T\vec c_2]_{\mathcal C}$.
		\item Compute the result of $T\mat{\alpha\\\beta}_{\mathcal C}$ and express the result in the
			$\mathcal C$ basis (i.e., as a vector of the form $\mat{?\\?}_{\mathcal C}$).
		\item Find a matrix for $T$ in the $\mathcal C$ basis.
		\item Find a matrix for $T$ in the standard basis.
	\end{parts}
	\begin{definition}[Similar Matrices]
		A matrix $A$ and a matrix $B$ are \emph{similar matrices}, denoted $A\sim B$, if
		$A$ and $B$ represent the same linear transformation but in possibly different bases.
		Equivalently, $A\sim B$ if there is an invertible matrix $X$ so that
		\[
			A=XBX^{-1}.
		\]
	\end{definition}



\newpage
\section*{Determinants}
	\begin{definition}[Unit $n$-cube]
		The unit $n$-cube is the $n$-dimensional cube with side length 1 and lower-left
		corner located at the origin.  That is 
		\[
			C_n = \left\{\vec x\in\R^n:\vec x=\sum_{i=1}^n \alpha_i\vec e_i\text{ for some }\alpha_1,\ldots,\alpha_n\in[0,1]\right\}=[0,1]^n.
		\]
	\end{definition}
	The volume of the unit $n$-cube is always 1.

	\question
	The picture shows what the linear transformation $T$ does to the unit square (i.e., the unit $2$-cube).

	\begin{center}
	\includegraphics[width=2.5in]{images/transform1b.pdf}
	\includegraphics[width=2.5in]{images/transform2b.pdf}
	\end{center}

	\vspace{-6em}
	\begin{parts}
		\item What is $T\mat{1\\0}$, $T\mat{0\\1}$, $T\mat{1\\1}$?
		\item Write down a matrix for $T$.
		\item What is the volume of the image of the unit square (i.e., the volume of $T(C_2)$)?  You may need
			to use trigonometry.
	\end{parts}
	
	\begin{definition}[Determinant]
	The \emph{determinant} of a linear transformation $X:\R^n\to \R^n$ is the 
	oriented volume of the image of the unit $n$-cube.  The determinant
	of a square matrix is the oriented volume of the parallelepiped 
	($n$-dimensional parallelogram) given by the column vectors or the row
	vectors.
	\end{definition}

	\question
	We know the following about the transformation $A$:
	\[
		A\mat{1\\0}=\mat{2\\0}\qquad\text{and}\qquad A\mat{0\\1}=\mat{1\\1}.
	\]
	\begin{parts}
	\item Draw $C_2$ and $A(C_2)$, the image of the unit square
			under $A$.
		\item Compute the area of $A (C_2)$.
		\item Compute $\det(A)$.
	\end{parts}

	\question
	Suppose $R$ is a rotation counterclockwise by $30^\circ$.
	\begin{parts}
	\item Draw $C_2$ and $R(C_2)$.
	\item Compute the area of $R(C_2)$.
		\item Compute $\det(R)$.
	\end{parts}
	
	\question
	We know the following about the transformation $F$:
	\[
		F\mat{1\\0}=\mat{0\\1}\qquad\text{and}\qquad F\mat{0\\1}=\mat{1\\0}.
	\]
	\begin{parts}
		\item What is $\det(F)$?
	\end{parts}

	\question
	\begin{itemize}
		\item $E_f$ is $I_{3\times 3}$ with the first two rows swapped.
		\item $E_m$ is $I_{3\times 3}$ with the third row multiplied by 6.
		\item $E_a$ is $I_{3\times 3}$ with $R_1\to R_1+2R_2$ applied.
	\end{itemize}

	\begin{parts}
		\item What is $\det(E_f)$?
		\item What is $\det(E_m)$?
		\item What is $\det(E_a)$?
		\item What is $\det(E_fE_m)$?
		\item What is $\det(4I_{3\times 3})$?
		\item What is $\det(W)$ where $W=E_fE_aE_fE_mE_m$?
	\end{parts}

	\question
	$U=\mat{1&2&1&2\\0&3&-2&4\\0&0&-1&0\\0&0&0&4}$
	\begin{parts}
		\item What is $\det(U)$?
		\item $V$ is a square matrix and rref$(V)$ has a row of zeros.
		 What is $\det(V)$?
		\item $P$ is projection onto the vector $\mat{-1\\-1}$. What is $\det(P)$?
	\end{parts}

	\question
	Suppose you know $\det(X)=4$.
	\begin{parts}
		\item What is $\det(X^{-1})$?
		\item Derive a relationship between $\det(Y)$
			and $\det(Y^{-1})$ for an arbitrary matrix $Y$.
		\item Suppose $Y$ is not invertible.  What is $\det(Y)$?
	\end{parts}

	After all this work with determinants, we see 
	that (like dot products) there is a geometric and an
	algebraic way of thinking about them, and they 
	\emph{determine} if a matrix is invertible.

	\question
	\begin{parts}
		\item The linear transformation $L:\R^3\to\R^3$ is a change of coordinates and $\det(L)=-4$.
			What is the volume form for this change of coordinates?
		\item Suppose $P:\R^2\to\R^2$ is the parameterization defined by $P\left(\mat{x\\y}\right)=\mat{1&2\\3&9}\mat{x\\y}+\mat{1\\1}$.
			Find the volume form for $P$.
		\item Suppose $p:\R^2\to\R^2$ is the parameterization defined by $p(r,\theta)=(r\cos\theta,r\sin\theta)$.  Find
			a linear approximation to $p$ at the point $(r_0,\theta_0)$.  Use determinants to compute the volume form for $p$
			at $(r_0,\theta_0)$.
	\end{parts}
	\begin{definition}[Jacobian]
		Let $p:\R^n\to\R^n$ be a parameterization.  Let $L_{\vec x_0}(\vec x) = J_{\vec x_0}\vec x+\vec q_{\vec x_0}$
		be the linear approximation to $p$ at the point $\vec x_0$.  The \emph{Jacobian} of $p$ at the point $\vec x_0$
		is defined to be
		\[
			\mathrm{Jacob}_{\vec x_0}(p) = \det(J_{\vec x_0}).
		\]
	\end{definition}


\newpage
\section*{Eigenvectors}

	\vspace{-.6cm}
	\begin{definition}[Eigenvector]
	For a linear transformation $X$, an \emph{eigenvector} for $X$ is a non-zero vector that doesn't
	change directions when $X$ is applied.  That is, $\vec v\neq \vec 0$ is an eigenvector for $X$ if
	\[
		X\vec v=\lambda \vec v
	\]
	for some scalar $\lambda$.  We call $\lambda$ the \emph{eigenvalue} 
	of $X$ corresponding
	to the eigenvector $\vec v$.
	\end{definition}
	\vspace{-.2cm}

	\question
	The picture shows what the linear transformation $T$ does to the unit square (i.e., the unit $2$-cube).
	
	\vspace{-1cm}
	\begin{center}
	\includegraphics[width=2in]{images/transform1b.pdf}
	\includegraphics[width=2in]{images/transform2b.pdf}
	\end{center}
	\vspace{-2.5cm}

	\begin{parts}
		\item Give an eigenvector for $T$.  What is the eigenvalue?
		\item Can you give another?
	\end{parts}


	\question
	For some matrix $A$,
	\vspace{-.2cm}
	\[
		A\mat{3\\3\\1}=\mat{2\\2\\2/3}\qquad\text{ and }\qquad B=A-\tfrac{2}{3}I.
	\]
	\vspace{-.4cm}
	\begin{parts}
		\item Give an eigenvector and a corresponding eigenvalue for $A$.
		\item What is $B\mat{3\\3\\1}$?
		\item What is the dimension of $\text{null}(B)$?
		\item What is $\det(B)$?
	\end{parts}

	\vspace{-.2cm}
	\question
	Let $C=\mat{-1&2\\1&0}$ and $E_\lambda = C-\lambda I$.
	\begin{parts}
		\item For what values of $\lambda$ does $E_\lambda$ have a non-trivial
			null space?
		\item What are the eigenvalues of $C$?
		\item Find the eigenvectors of $C$.
	\end{parts}
	
	\begin{definition}[Characteristic Polynomail]
	For a matrix $A$, the \emph{characteristic polynomial} of $A$ is
	\[
		\chr(A)=\det(A-\lambda I).
	\]
	\end{definition}
	\vspace{-.4cm}
	
	\question
	Let $D=\mat{1&2\\3&0}$.
	\begin{parts}
		\item Compute $\chr(D)$.
		\item Find the eigenvalues of $D$.
	\end{parts}

	\vspace{-.3cm}
	\question
	\vspace{-.2cm}
	Suppose $\chr(E)=\lambda(\lambda -2)(\lambda +3)$ for some unknown $3\times 3$
	matrix $E$.
	\begin{parts}
		\item What are the eigenvalues of $E$?
		\item Is $E$ invertible?
		\item What is $\nnul(E)$, $\nnul(E-3I)$, $\nnul(E+3I)$?
	\end{parts}

	\question
	Consider
	\[
		A=\mat{1&0&1\\0&1&1\\1&1&0}\qquad
		\vec v_1=\mat{1\\1\\1}\qquad
		\vec v_2=\mat{1\\1\\-2}\qquad
		\vec v_3=\mat{-1\\1\\0}
	\]
	and notice that $\vec v_1,\vec v_2,\vec v_3$ are eigenvectors for $A$.
	\begin{parts}
		\item Find the eigenvalues of $A$.
		\item Find the characteristic polynomial of $A$.
		\item Compute $A\vec w$ where $w=2\vec v_1-\vec v_2$.
		\item Compute $A\vec u$ where $\vec u=a\vec v_1+b\vec v_2+c\vec v_3$ for
			unknown scalar coefficients $a,b,c$.
	\end{parts}
	Notice that $\mathcal V=\{\vec v_1,\vec v_2,\vec v_3\}$ is a basis for $\R^3$.
	\begin{parts}[resume]
	\item If $[\vec x]_{\mathcal V}=\mat{1\\3\\4}$ is $\vec x$ written in the $\mathcal V$ basis,
		compute $A\vec x$ in the $\mathcal V$ basis.
	\end{parts}
	
	\question
	The transformation $P^{-1}$ takes vectors in the standard basis and outputs
	vectors in their $\mathcal V$-basis representation (where $\mathcal V$ is from above).  
	\begin{parts}
		\item Describe in words what $P$ does.
		\item Describe how you can use $P$ and $P^{-1}$ to easily compute
			$A\vec y$ for any $\vec y\in \R^3$.
		\item Can you find a matrix $D$ so that
			\[
				PDP^{-1}=A?
			\]
		\item $[\vec x]_{\mathcal V}=\mat{1\\3\\4}$.  Compute $A^{100}\vec x$.
	\end{parts}


	\question
	For an $n\times n$ matrix $T$, suppose its eigenvectors $\{\vec v_1,\ldots \vec v_n\}$
	form a basis for $\R^n$.  Let $\lambda_1,\ldots,\lambda_n$ be the corresponding
	eigenvalues.
	

	\begin{parts}
	\item Is $T$ diagonalizable (i.e., similar to a diagonal matrix)?  If so, explain how to obtain its diagonalized form.
		\item What if one of the eigenvalues of $T$ is zero?  Is $T$ diagonalizable?
		\item What if the eigenvectors of $T$ did not form a basis for $\R^n$.
			Would $T$ be diagonalizable?
	\end{parts}

	\begin{definition}[Eigenspace]
	Let $A$ be a matrix with eigenvalues $\{\lambda_1,\ldots,\lambda_m\}$.  The
	\emph{eigenspace} of $A$ corresponding to the eigenvalue $\lambda_i$ is the
	null space of $A-\lambda_i I$.  That is, it is the space spanned by all eigenvectors
	that have the eigenvalue $\lambda_i$.

	The \emph{geometric multiplicity} of an eigenvalue $\lambda_i$ is the dimension
	of the eigenspace corresponding to $\lambda_i$.  The \emph{algebraic multiplicity}
	of $\lambda_i$ is the number of times $\lambda_i$ occurs as a root of the
	characteristic polynomial of $A$ (i.e., the number of times $x-\lambda_i$
	occurs as a factor).
	\end{definition}

	\question
	Define $F=\mat{1&1\\0&1}$.
	\begin{parts}
		\item Is $F$ diagonalizable?  Why or why not?
		\item What is the geometric and algebraic multiplicity of each eigenvalue
			of $F$?
		\item Suppose $A$ is a matrix where the geometric multiplicity of one of its eigenvalues
			is smaller than the algebraic multiplicity of the same eigenvalue.  Is
			$A$ diagonalizable?  What if all the geometric and algebraic multiplicities
			match?
	\end{parts}

\newpage

\section*{Orthogonality}
	\begin{definition}[Orthogonal \& Orthonormal]
		A set of vectors is \emph{orthogonal} if every pair of vectors
		in the set is orthogonal.  A set of vectors is \emph{orthonormal}
		if it is both an orthogonal set and every vector is a unit vector.
	\end{definition}

	\question
	\[
		\mathcal B=\{\vec b_1,\vec b_2\}\qquad\vec b_1=\mat{1/2\\\sqrt{3}/2}
		\qquad \vec b_2=\mat{-\sqrt{3}/2\\1/2}
	\]
	The matrix $A=[\vec b_1|\vec b_2]$ takes vectors in the $\mathcal B$ basis
	and rewrites them in the standard basis.
	\begin{parts}
		\item What does $A^{-1}$ do?
		\item Find a matrix $B$ that takes vectors in the standard basis
			and rewrites them in the $\mathcal B$ basis.
		\item Write $\vec x=\mat{1\\2}_S$ in the $\mathcal B$ basis.
		\item What is the relationship between $A$ and $B$?
	\end{parts}

	\begin{definition}[Orthogonal Matrix]
		An \emph{orthogonal matrix} is a square matrix whose columns are
		orthonormal (Yes, a better name would be orthonormal matrix, but that
		is not the term the rest of the world uses).
	\end{definition}

	\question
	Suppose $X=[\vec x_1|\vec x_2|\vec x_3|\vec x_4]$ is an orthogonal matrix.
	\begin{parts}
		\item What is the shape of $X$ (i.e., it is a what$\times$what matrix)?
		\item Compute $X^TX$.
		\item What is $X^{-1}$?
	\end{parts}

	\question
	\[
		Y=\mat{1&1&1&-1\\1&-1&-1&-1\\1&1&-1&1\\1&-1&1&1}
	\]
	\begin{parts}
		\item Is $Y$ an orthogonal matrix?
		\item Fix $Y$ so it is an orthogonal matrix.  Call the new matrix $X$.
		\item Compute $X^{-1}$.
		\item Compute $Y^{-1}$.
		\item Compute $|\det(X)|$ and $|\det(Y)|$ (the absolute value of
			the determinant of $X$ and $Y$).
	\end{parts}

	Matrix equations involving orthogonal matrices are easy to solve because the
	inverse of an orthogonal matrix is so easy to compute!
	
	\question
	Let $A=[\vec a_1|\vec a_2|\vec a_3|\vec a_4]$ be an orthogonal matrix.
	\begin{parts}
		\item Explain why 
			$\vec x=\mat{\vec a_1\cdot \vec b\\
				     \vec a_2\cdot \vec b\\
			     	     \vec a_3\cdot \vec b\\
			     	     \vec a_4\cdot \vec b}$ is a solution to $A\vec x=\vec b$.
		\item Find scalars $a,b,c,d$ so $\vec b=a\vec a_1+b\vec a_2+c\vec a_3+d\vec a_4$
			(your answers will have variables in them).
	\end{parts}

	Orthogonal matrices also allow us to compute projections quite easily.

	\begin{definition}[Orthogonal Projection]
		If $V$ is a subspace of $\R^n$, the \emph{projection}
		(sometimes called the orthogonal projection) of $\vec x$ onto $V$
		is the closest point in $V$ to $\vec x$. We notate the projection
		of $\vec x$ onto $V$ as $\proj_V\vec x$.
	\end{definition}

	Projections are normally hard to compute and a priori might require some sort
	of calculus-style optimization to find.  However, from geometry we know that 
	if we travel from $\proj_V \vec x$ to $\vec x$, we should always trace out a path
	perpendicular to $V$.  Otherwise, we could find a point in $V$ that was slightly closer
	to $\vec x$, violating the definition of $\proj_V \vec x$.  Thus, orthogonality
	will be our savior.

	\question
	Let $\mathcal S=\{\vec e_1,\vec e_2,\vec e_3\}$ be the standard basis.
	\begin{parts}
		\item If $\vec x=1\vec e_1+2\vec e_2+3\vec e_3$, find the projection of $\vec x$
			onto the $xy$-plane.
	\end{parts}
	Suppose $\mathcal B=\{\vec b_1,\vec b_2,\vec b_3\}$ is an orthonormal basis for $\R^3$.
	\begin{parts}[resume]
		\item If $\vec y=3\vec b_1-2\vec b_2+2\vec b_3$, find the projection of $\vec y$
			onto $\span\{\vec b_1,\vec b_3\}$.
	\end{parts}
	Suppose $\mathcal C=\{\vec c_1,\vec c_2,\vec c_3\}$ is a basis for $\R^3$ with
	\[
		\|\vec c_1\| = 
		\|\vec c_2\| = 
		\|\vec c_3\| = 1\qquad \vec c_1\cdot \vec c_2=0\qquad \vec c_1\cdot \vec c_3=0
		\qquad \vec c_2\cdot \vec c_3=\sqrt{2}/2.
	\]
	\vspace{-.35in}
	\begin{parts}[resume]
		\item If $\vec z=5\vec c_1+2\vec c_2-\vec c_3$, find the projection of $\vec z$
			onto $\span\left\{\vec c_1,\vec c_2\right\}$.
	\end{parts}

	\question
	Let's put this all together.  
	$\mathcal B=\left\{\mat{2\\1\\1},\mat{1\\-1\\-1},\mat{0\\1\\-1}\right\}$ is an
	orthogonal basis for $\R^3$.  Let $\mathcal P$ be the plane defined
	by
	\[
		0x+y-z=0.
	\]
	\begin{parts}
		\item Write $\mathcal P$ in vector form (Hint: think about the vectors
			listed in the $\mathcal B$ basis).
		\item Find an orthonormal basis $\mathcal C=\{\vec c_1,\vec c_2,\vec c_3\}$
			for $\R^3$ so $\mathcal P=\span\{\vec c_1,\vec c_2\}$.
		\item Let $\vec x=\mat{1\\2\\3}$.  Find $\proj_{\mathcal P}\vec x$.
	\end{parts}

%\newpage
\subsection*{Gram-Schmidt Orthogonalization}
	We've seen how useful orthonormal bases are.  The incredible thing is that we can 
	turn any basis into an orthonormal basis through a process called
	Gram-Schmidt orthogonalization.

	\question
	Let $\vec a=\mat{1\\2}$ and $\vec b=\mat{3\\1}$.
	\begin{parts}
		\item Draw $\vec a$ and $\vec b$ and find $\vec w=\proj_{\vec b}\vec a$.
		\item Add $\vec c=\vec a-\vec w$ to your drawing.  What is the angle between
			$\vec c$ and $\vec b$.
		\item Can you write $\vec a$ as the sum of two vectors, one in 
			the direction of $\vec b$ and one orthogonal to $\vec b$?
			If so, do it.
	\end{parts}

	\question
	Let $\vec a=\mat{1\\2\\6}$ and $\vec b=\mat{1\\1\\-1}$.
	\begin{parts}
		\item Write $\vec a=\vec u+\vec v$ where $\vec u$ is parallel to
			$\vec b$ and $\vec v$ is orthogonal to $\vec b$.
		\item Find an orthonormal basis for $\span\{\vec a,\vec b\}$.
	\end{parts}

	With two vectors, making an orthonormal set without changing the span
	is quite easy.  With more vectors, it is only slightly harder.

	\begin{definition}[Gram-Schmidt Process]
		The \emph{Gram-Schmidt} orthogonalization procedure
		takes in a set of vectors and outputs a set of orthonormal vectors
		with the same span.  The idea is to iteratively produce a set of
		vectors where each new vector you produce is orthogonal to the previous vectors.

		The algorithm is as follows: Let $\{ v_1,\ldots, v_n\}$ be a set of 
		vectors.  Produce a set $\{ v_2',\ldots, v_n'\}$ that is orthogonal
		to $ v_1$ by subtracting off the respective projections
		of $ v_2,\ldots, v_n$
		onto $ v_1$.  Next, produce a set $\{ v_3'',\ldots, v_n''\}$
		orthogonal to both $ v_1$ and $ v_2'$ by subtracting off the
		respective projections
		onto $ v_2'$.  Continue this process until you have a set
		$V=\{ v_1, v_2', v_3'', v_4''',\ldots\}$ that is orthogonal.
		Finally, normalize $V$ so all vectors have unit length.
	\end{definition}

	\question
	Let $\vec x_1=\mat{1\\-1\\-1\\1}$, $\vec x_2=\mat{2\\1\\0\\1}$, and 
	$\vec x_3=\mat{2\\2\\1\\2}$.
	\begin{parts}
		\item Use the Gram-Schmidt procedure to find an orthonormal basis for 
			$\span\{\vec x_1,\vec x_2,\vec x_3\}$.
		\item Find an orthonormal basis $\mathcal V=\{\vec v_1,\vec v_2,\vec v_3,\vec v_4\}$
			for $\R^4$ so that $\span\{\vec v_1,\vec v_2,\vec v_3\}=
			\span\{\vec x_1,\vec x_2,\vec x_3\}$.
	\end{parts}
	Let $R=\mat{1&-1&-1&1\\2&1&0&1\\2&2&1&2}$.
	\begin{parts}[resume]
		\item Find an orthonormal basis for the row space of $R$.
		\item Find the null space of $R$ (Hint, you've already done the work, so
			there is no need to row reduce).
	\end{parts}

	\question
	Let
	\[
		\vec y_1=\mat{1\\1\\2}\qquad 
		\vec y_2=\mat{-1\\-1\\2}\qquad
		\vec y_3=\mat{1\\1\\6}.
	\]
	\begin{parts}
		\item Find an orthonormal basis $\mathcal W$ so that $\span\mathcal W=
			\span\{\vec y_1,\vec y_2,\vec y_3\}$.
	\end{parts}

	\begin{definition}[Orthogonal Complement]
		The \emph{orthogonal complement} of a subspace $V$ is written
		$V^\perp$ and defined as
		\[
			V^\perp=\{\vec x:\vec x\text{ is orthogonal to }V\}.
		\]
		\vspace{-.2in}
	\end{definition}

	\begin{parts}[resume]
		\item Find the orthogonal complement of $\span \mathcal W$.
		\item Write $\vec v=\mat{1\\0\\1}$ in the form $\vec v=\vec r+\vec n$ where 
			$\vec r\in\span\mathcal W$ and $\vec n\in(\span \mathcal W)^\perp$.
	\end{parts}


\subsection*{$QR$ Decomposition}

	\begin{definition}[$QR$ Decomposition]
		For a matrix $A$, we can rewrite $A=QR$ where $Q$ is an
		orthogonal matrix and $R$ is an upper triangular matrix.  Writing
		$A$ as $QR$ is called the \emph{$QR$ decomposition} of $A$.
	\end{definition}

	\question
	Suppose $A,B,C$ are square matrices and $C=AB$.
	\begin{parts}
		\item How do the column spaces of $A$ and $C$ relate?
		\item How do the column spaces of $B$ and $C$ relate?
	\end{parts}

	\question
	$\mathcal V=\{\vec v_1,\vec v_2,\vec v_3\}$ forms a basis for $\R^3$.
	When we apply the Gram-Schmidt process to $\mathcal V$, we get
	\[
		\begin{array}{rl}
			q_1' &=\vec v\\
			q_2' &= \vec v_2-\frac{1}{2}\vec v_2\\
			q_3' &= \vec v_3-\vec v_1+2\vec v_2
		\end{array}
	\]
	form an orthogonal set.  Normalizing we get
	\[
		\begin{array}{rl}
			\vec q_1 &= 2q_1'\\
			\vec q_2 &= 3q_2'\\
			\vec q_3 &=\frac{1}{2}q_3'
		\end{array}
	\]
	form an orthonormal set.
	\begin{parts}
		\item Write $\vec v_1$ as a linear combination of $\vec q_1,\vec q_2,\vec q_3$.
		\item Write $\vec v_2$ as a linear combination of $\vec q_1,\vec q_2,\vec q_3$.
		\item Write $\vec v_3$ as a linear combination of $\vec q_1,\vec q_2,\vec q_3$.
	\end{parts}
	Define $A=[\vec v_1|\vec v_2|\vec v_2]$ and $Q=[\vec q_1|\vec q_2|\vec q_3]$.
	\begin{parts}[resume]
		\item Find a matrix $R$ so that $A=QR$.
	\end{parts}
	
	We've just discovered one process to find the $QR$ decomposition of a matrix.
	It's really as simple as doing Gram-Schmidt and keeping track of your coefficients.
	Now, we have another way to the matrix equation $A\vec x=\vec b$.  If we do a $QR$
	decomposition and exploit the fact that $Q^{-1}=Q^T$, we have
	\[
		A\vec x=QR\vec x=\vec b\qquad\implies\qquad R\vec x=Q^T\vec b
	\]
	and $R$ is a triangular matrix, so we can just do back substitution! (It turns
	out that if you solve systems this way, there is less rounding error than if you
	use row reduction.)

\subsection*{Symmetric Matrices}
	When you're new to Linear Algebra, learning lots of new concepts and algorithms,
	it's sometimes hard to grasp the significance of certain properties of a matrix.

	Symmetric matrices are easy to forget at first, but they have many profound 
	properties (not to mention they are one of the key concepts of Quantum Mechanics).

	\question
	Let $A$ be a symmetric matrix and let $\vec v$ be an eigenvector with eigenvalue
	3 and $\vec w$ be an eigenvector with eigenvalue 4.  Note, for this problem,
	we are thinking of $\vec v$ and $\vec w$ as column vectors.
	\begin{parts}
		\item Write $A\vec v$, $\vec v^TA^T$, $\vec v^TA$, $A\vec w$, $\vec w^TA^T$, 
		and $\vec w^TA$ in terms of $\vec v$, $\vec w$ and scalars.
		\item How do $\vec v^T\vec w$ and $\vec w^T\vec v$ relate?
		\item What should $\vec v^TA\vec w$ be in terms of $\vec v^T$ and
			$\vec w$? (Note, you could compute $(\vec v^TA)\vec w$
			or $\vec v^T(A\vec w)$.  Better do both to be safe).
		\item What can you conclude about $\vec v^T\vec w$?  How about
			$\vec v\cdot \vec w$?
	\end{parts}

	We've just deduced that all eigenspaces of a symmetric matrix are orthogonal! On
	top of that, symmetric matrices always have a basis of eigenvectors.  That means
	that not only can you always diagonalize a symmetric matrix, but you can 
	\emph{orthogonally} diagonalize a symmetric matrix. (i.e. if $A$ is symmetric,
	then $A=QDQ^T$ where $Q$ is orthogonal and $D$ is diagonal).  This is like the 
	best of all worlds in one!

\end{document}
