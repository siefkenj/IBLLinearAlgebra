From here on out, we will only be considering linear transformations with the same domain
and codomain (i.e., transformations $\mathcal T:\R^n\to\R^n$). Why? Because that will allow us
to \emph{compare} input and output vectors. By comparing inputs and outputs, we
may describe a linear transformation as a stretch, twist, shear, rotation, projection, or some combination
of all of these operations. 

\begin{center}
	\begin{tikzpicture}
		\begin{scope}[cm={1,0,0,1,(0,0)}, black]
			\node[transform shape, black] at (.5,.5) {Box};
			\fill[opacity=.3] (0,0) rectangle (1,1) +(-.5,0)node[opacity=1, above] {Untransformed};
			\draw[thick, ->] (0,0) -- (1,0);
			\draw[thick, densely dotted, ->] (0,0) -- (0,1);
		\end{scope}
		\begin{scope}[cm={1,0,-1,1,(3,0)}, mygreen]
			\node[transform shape, black] at (.5,.5) {Box};
			\fill[mygreen, opacity=.3] (0,0) rectangle (1,1)  +(-.5,0) node[opacity=1, above] {Shear};
			\draw[thick, ->] (0,0) -- (1,0);
			\draw[thick, densely dotted, ->] (0,0) -- (0,1);
		\end{scope}
		\begin{scope}[cm={1, 0, 0, 1,(5,.13)}, Blue]
			\draw[very thick, opacity=.5] (0,0) -- (1.15,.77) +(-.5,0)node[opacity=1, above] {Project};
			\draw[thick, ->] (0,0) -- (.69,.46);
			\draw[thick, densely dotted, ->] (0,0) -- (.46, .31);
		\end{scope}
		\begin{scope}[cm={.936, .352, -.352, .936,(7.5,-.1)}, myorange]
			\node[transform shape, black] at (.5,.5) {Box};
			\fill[opacity=.3] (0,0) rectangle (1,1)  +(-.5,0) node[opacity=1, above] {Rotate};
			\draw[thick, ->] (0,0) -- (1,0);
			\draw[thick, densely dotted, ->] (0,0) -- (0,1);
		\end{scope}
		\begin{scope}[cm={3,0,0,1,(9.4,0)}, mypink]
			\node[transform shape, black] at (.5,.5) {Box};
			\fill[opacity=.3] (0,0) rectangle (1,1)  +(-.5,0) node[opacity=1, above] {Stretch};
			\draw[thick, ->] (0,0) -- (1,0);
			\draw[thick, densely dotted, ->] (0,0) -- (0,1);
		\end{scope}
	\end{tikzpicture}
\end{center}

It's the stretched vectors that we're most interested in now. If $\mathcal T$ stretches the vector $\vec v$,
then $\mathcal T$, in that direction, can be described by $\vec v\mapsto \alpha\vec v$, which is an easy-to-understand
linear transformation. The ``stretch'' directions for a linear transformation have a special name---\emph{eigen directions}---and
the vectors that are stretched are called \emph{eigenvectors}.

\SavedDefinitionRender{Eigenvector}

The word \emph{eigen} is German for characteristic, representative, or intrinsic, and
we will see that eigenvectors provide one of the best contexts in which to understand a linear transformation.

\begin{example}
	Let $\mathcal P:\R^2\to\R^2$ be projection onto the line $\ell$ given by $y=x$.
	Find the eigenvectors and eigenvalues of $\mathcal P$.

	We are looking for vectors $\vec v\neq \vec 0$ such that $\mathcal P\vec v=\lambda \vec v$ for some $\lambda$.
	Since $\mathcal P(\ell)=\ell$, we know for any $\vec v\in \ell$
	\[
		\mathcal P(\vec v)=1\vec v=\vec v.
	\]
	Therefore, any non-zero multiple of $\mat{1\\1}$ is an eigenvector for $\mathcal P$ with corresponding
	eigenvalue $1$.

	By considering the null space of $\mathcal P$, we see, for example,
	\[
		\mathcal P\mat{1\\-1}=\mat{0\\0}=0\mat{1\\-1},
	\]
	and so $\mat{1\\-1}$ and all its non-zero multiples are eigenvectors of $\mathcal P$ with corresponding
	eigenvalue $0$.
\end{example}

\Heading{Finding Eigenvectors}

Sometimes you can find the eigenvectors/values of a linear transformation just by thinking about it.
For example, for reflections, projections, and dilations, the eigen directions
are geometrically clear. However, for an arbitrary matrix transformation, it may not be obvious.

Our goal now will be to see if we can leverage linear algebra knowledge to find eigenvectors/values.
So that we don't have to switch back and forth
between thinking about linear transformations and thinking about matrices, let's just think about matrices for now.

Let $M$ be a square matrix. The vector $\vec v\neq \vec 0$
is an eigenvector for $M$ if and only if there exists a scalar $\lambda$ so that
\begin{equation}
	\label{EQEIGEN}
	M\vec v=\lambda \vec v.
\end{equation}
Put another way, $\vec v\neq \vec 0$ is an eigenvector for $M$ if and only if
\[
	M\vec v-\lambda \vec v=(M-\lambda I)\vec v=\vec 0.
\]
The middle equation provides a key insight. The operation $\vec v\mapsto M\vec v-\lambda\vec v$ can be achieved
by multiplying $\vec v$ by the single matrix $E_\lambda=M-\lambda I$.

Now we have that $\vec v\neq \vec 0$ is an eigenvector for $M$ if and only if
\[
	E_\lambda \vec v=(M-\lambda I)\vec v = M\vec v-\lambda \vec v=\vec 0,
\]
or, phrased another way, $\vec v$ is a non-zero vector satisfying $\vec v\in \Null(E_\lambda)$.

We've reduced the problem of finding eigenvectors/values of $M$ to finding the null space of $E_\lambda$,
a related matrix.

\Heading{Characteristic Polynomial}

Let $M$ be an $n\times n$ matrix and define $E_\lambda=M-\lambda I$. Every eigenvector for
$M$ must be in the null space of $E_\lambda$ for some $\lambda$. However, because eigenvectors
must be non-zero, the only chance we have of finding an eigenvector is if $\Null(E_\lambda)\neq \Set{\vec 0}$.
In other words, we would like to know when $\Null(E_\lambda)$ is \emph{non-trivial}.

We're well equipped to answer this question. Because $E_\lambda$ is an $n\times n$ matrix, we know $E_\lambda$ has
a non-trivial null space if and only if $E_\lambda$ is not invertible which is true if and only if $\det(E_\lambda)=0$.
Every $\lambda$ defines a different $E_\lambda$ where eigenvectors could be hiding. By viewing $\det(E_\lambda)$
\emph{as a function of $\lambda$}, we can use our mathematical knowledge of single-variable functions to 
figure out when $\det(E_\lambda)=0$.

The quantity $\det(E_\lambda)$, viewed as a function of $\lambda$, has a special name---it's
called the \emph{characteristic polynomial}\footnote{ This time the term is traditionally given the English name, rather
than being called the \emph{eigenpolynomial}.}.

\SavedDefinitionRender{CharacteristicPolynomial}

\begin{example}
	Find the characteristic polynomial of $A=\mat{1&2\\3&4}$.
	
	By the definition of the characteristic polynomial of A, we have
	\begin{align*}
	    \Char(A) &=\det(A-\lambda I)\\
	            &=\det\left(\mat{1&2\\3&4}-\mat{\lambda&0\\0&\lambda}\right)=\det\left(\mat{1-\lambda&2\\3&4-\lambda}\right)\\
	            &=(1-\lambda)(4-\lambda) - 6=\lambda^2 -5\lambda-2.
	\end{align*}
\end{example}
For an $n\times n$ matrix $A$, $\Char(A)$ has some nice properties.
\begin{itemize}
	\item $\Char(A)$ is a polynomial\footnote{ A priori, it's not obvious that $\det(A-\lambda I)$
	should be a polynomial as opposed to some other type of function.}.
	\item $\Char(A)$ has degree $n$.
	\item The coefficient of the $\lambda^n$ term in $\Char(A)$ is $\pm1$.
	\item $\Char(A)$ evaluated at $\lambda = 0$ is $\det(A)$.
	\item The roots of $\Char(A)$ are precisely the eigenvalues of $A$.
\end{itemize}
We will just accept these properties as facts, but each of them can be proved with the tools we've developed.

\Heading{Using the Characteristic Polynomial to find Eigenvalues}

With the characteristic polynomial in hand, finding eigenvectors/values becomes easier.

\begin{example}
	Find the eigenvectors/values of $A=\mat{1&2\\3&2}$.

	Like the previous example, we first compute $\Char(A)$.
	\begin{align*}
	    \Char(A)&=\det\left(\mat{1-\lambda&2\\3&2-\lambda}\right)\\
	            &=(1-\lambda)(2-\lambda)-6=\lambda^2-3\lambda-4=(4-\lambda)(-1-\lambda)
	\end{align*}
	Next, we solve for when
	$\chr(A)=0$ to find eigenvalues, 
	which are $\lambda_1=-1$ and $\lambda_2=4$. 
	
	We know non-zero vectors in $\Null(A-\lambda_1I)$ are eigenvectors with eigenvalue $-1$. Computing, 
	\[
	\Null(A-\lambda_1I) = \Null\left(\mat{2&2\\3&3}\right)=
	\Span\Set*{\mat{1\\-1}},
	\]
	And so the eigenvectors of $A$ corresponding to eigenvalue $\lambda_1=-1$
	are the non-zero multiples of $\mat{1\\-1}$.
	
	Similarly, for $\lambda_2=4$, we compute
	\[
	\Null(A-\lambda_2I) = \Null\left(\mat{-3&2\\3&-2}\right)=
	\Span\Set*{\mat{2\\3}},
	\]
	and so the eigenvectors for $A$ with eigenvalue $4$ are the non-zero multiples of $\mat{2\\3}$.
	
\end{example}

Using the characteristic polynomial, we can show that every eigenvalue
for a matrix is a root of some polynomial (the characteristic polynomial).
In general, finding roots of polynomials is a hard problem\footnote{ In fact,
numerically approximating eigenvalues turns out to be easier than finding roots
of a polynomial, so many numerical root finding algorithms actually create a matrix
with an appropriate characteristic polynomial and use numerical linear
algebra to approximate its roots.}, and it's not one we will focus on. However, it's
handy to have the quadratic formula in your back pocket for factoring particularly
stubborn polynomials.

\begin{example}
	Find the eigenvectors/values of $A=\mat{1&2\\3&4}$.

	First, we find the roots of $\Char(A)$ by setting it to $0$.
	\begin{align*}
	    \Char(A)&=\det\left(\mat{1-\lambda&2\\3&4-\lambda}\right)\\
	            &=(1-\lambda)(4-\lambda)-6=\lambda^2-5\lambda-2=0
	\end{align*}
	By the quadratic formula\footnote{ Recall that the
	roots of $ax^2+bx+c$ are given by $\frac{-b\pm\sqrt{b^2-4ac}}{2a}$.}, we find that
	\[\lambda_1=\frac{5-\sqrt{33}}{2}\qquad\lambda_2=\frac{5+\sqrt{33}}{2}\]
	are the roots of $\Char(A)$.
	
	
	Following the procedure outlined above, we need to
	find $\Null(A-\lambda_1I)$ and $\Null(A-\lambda_2I)$.
	
	We will start by applying row reduction to $A-\lambda_1I$.
	\begin{align*}
	    \matc{1-\frac{5-\sqrt{33}}{2}&2\\3&4-\frac{5-\sqrt{33}}{2}} & \to
	    \matc{\frac{-3+\sqrt{33}}{2}&2\\3&\frac{3+\sqrt{33}}{2}} \to \mat{1&\frac{4}{-3+\sqrt{33}}\\1&\frac{3+\sqrt{33}}{6}}\\
	    & \to \matc{1&\frac{4(3+\sqrt{33})}{(-3+\sqrt{33})(3+\sqrt{33})}\\1&\frac{3+\sqrt{33}}{6}} \to \mat{1&\frac{3+\sqrt{33}}{6}\\1&\frac{3+\sqrt{33}}{6}}\\
	    & \to \matc{1&\frac{3+\sqrt{33}}{6}\\0&0}
	\end{align*}
	Thus, we conclude that the eigenvectors with eigenvalue $\frac{5-\sqrt{33}}{2}$ are the non-zero multiples of 
	$\matc{\frac{3+\sqrt{33}}{6}\\-1}$. 
	Similarly, the eigenvectors with eigenvalue $\frac{5\sqrt{33}}{2}$ 
	are the non-zero multiples of $\matc{\frac{3-\sqrt{33}}{6}\\-1}$.
\end{example}

\Heading{Transformations without Eigenvectors}

Are there linear transformations without eigenvectors? Well, it
depends on exactly what you mean. Let $\mathcal R:\R^2\to\R^2$
be rotation counter-clockwise by $90^\circ$. Are there any non-zero
vectors that don't change direction when $\mathcal R$ is applied?
Certainly not.

Let's examine further. We know $M_R=\mat{0&-1\\1&0}$ is a matrix for $\mathcal R$,
and
\[
	\Char(M_R) = \lambda^2+1.
\]
The polynomial $\lambda^2+1$ has no real roots, which means that $M_R$ (and $\mathcal R$) have
no real eigenvalues. However, $\lambda^2+1$ does have \emph{complex} roots of $\pm i$.
So far, we've always thought of scalars as real numbers, but if we allow complex numbers as
scalars and view $\mathcal R$ as a transformation from $\mathbb C^2\to\mathbb C^2$, it would have eigenvalues
and eigenvectors.

Complex numbers play an invaluable role in advanced linear algebra and applications of linear
algebra to physics. We will leave the following theorem as food for thought\footnote{ The theorem
is a direct corollary of the fundamental theorem of algebra.}.

\begin{theorem}
	If $A$ is a square matrix, then $A$ always has an eigenvalue provided complex eigenvalues are permitted.
\end{theorem}
