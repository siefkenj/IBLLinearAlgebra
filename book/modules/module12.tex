
We should think of transformations or functions as machines
that perform some manipulation of their input and then give an output. This perspective 
allows us to divide functions into two natural categories: those that can be undone and
those that cannot. The official term for a function that can be undone is an
\emph{invertible} function.

\Heading{Invertible Functions}

The simplest function is the \emph{identity function}.

\SavedDefinitionRender{IdentityFunction}

The identity function is the function that does nothing to its input\footnote{ Technically, for every
set there exists a unique identity function with that set as the domain/codomain, but we won't
belabor this point.}. When doing precise mathematics, we often prove a function or composition of functions
does nothing to its input by showing it is \emph{equal} to the identity function\footnote{ This is similar to saying that
we know $\vec x=\vec y$ if and only if $\vec x-\vec y=\vec 0$.}.

In plain terms, a function is invertible if it can be undone. More precisely a function
is invertible if there exists an inverse function that when composed with the original function
produces the identity function and vice versa.

\SavedDefinitionRender{InverseFunction}

Let's consider an example. You have some money in your 
pockets. Let $l:\Set{\text{nickels in left pocket}}\to \N$
be the function that adds up the value of all the nickels in your
left pocket. Let $r:\Set{\text{nickels in either pocket}}\to \N$
be the function that adds up the value of all the nickels in both of
your pockets. In this case, $l$ would be invertible---if you know that
$l(\text{\# nickels})=25$, you must have had $5$ nickels in your left pocket. We can write
down a formula for $l^{-1}$ as
\[
	l^{-1}(n)=\frac{n}{5}.
\]

However, $r$ is not invertible. If $r(\text{\# nickels})=25$, you \emph{might}
have had $5$ nickels in your left pocket, but you might have $3$ nickels in your left pocket
and $2$ in your right. We just don't know, so no inverse to $r$ can exist.

What we've just learned is that for a function to be invertible, it must be \emph{one-to-one}.

\SavedDefinitionRender{Onetoone}

Whenever a function $f$ is one-to-one, there exists a function $g$ so that $g\circ f=\Ident$.
However, this is not enough to declare that $f$ is invertible\footnote{ In this situation, we say that
$f$ is \emph{left}-invertible.} because we also need $f\circ g=\Ident$. To ensure this, we need
$f$ to be \emph{onto}.

\SavedDefinitionRender{Onto}

Every invertible function is both one-to-one and onto, and every one-to-one and onto function
is invertible. And, as we will learn, this has implications for the rank and nullity of linear transformations.

\Heading{Invertibility and Linear Transformations}

Let's now focus on linear transformations. We know that a linear transformation $\mathcal T:\R^n\to\R^m$ is
invertible if and only if it is one-to-one and onto. 

If $\mathcal T$ is one-to-one, that means that distinct inputs to $\mathcal T$ yield distinct outputs. In other words,
the solution to $\mathcal T(\vec x)=\vec b$ is always unique. But, the set of all solutions to $\mathcal T(\vec x)=\vec b$
can be expressed as
\[
	\Null(\mathcal T)+\Set{\vec p}.
\]
Therefore, $\mathcal T$ is one-to-one if and only if $\Nullity(\mathcal T)=0$.
If $\mathcal T$ is onto, then $\Range(\mathcal T)=\R^m$ and so $\Rank(\mathcal T)=m$. 

Now, suppose $\mathcal T$ is one-to-one and onto. By the rank-nullity theorem,
\[
	\Rank(\mathcal T)+\Nullity(\mathcal T) = 0+m=m=n=\Dim(\text{domain of $\mathcal T$}),
\]
and so $\mathcal T$ has the same domain and codomain (at least a domain and codomain of the same dimension).

Using the rank-nullity theorem\index{Rank-nullity theorem!for linear transformations}, we can start developing a list of properties that are equivalent to invertibility of a
linear transformation.
\begin{itemize}
	\item $\mathcal T:\R^n\to\R^m$ is invertible if and only if $\Nullity(\mathcal T)=0$ and $\Rank(\mathcal T)=m$.
	\item $\mathcal T:\R^n\to\R^m$ is invertible if and only if $m=n$ and $\Nullity(\mathcal T)=0$.
	\item $\mathcal T:\R^n\to\R^m$ is invertible if and only if $m=n$ and $\Rank(\mathcal T)=m$.
\end{itemize}

\begin{example}
	Let $\mathcal P:\R^2\to\R^2$ be projection onto the $x$-axis and let $\mathcal R:\R^2\to\R^2$ be rotation counter-clockwise
	by $15^\circ$. Classify each of $\mathcal P$ and $\mathcal R$ as invertible or not.
	
	\medskip
	Notice that $\mathcal P(\yhat)=\mathcal P(2\yhat)=\vec 0$, therefore $\mathcal P$ is not one-to-one and so is not invertible.
	
	\medskip
	Let $\mathcal Q: \R^2\to\R^2$ be rotation clockwise by $15^\circ$. $\mathcal R$ and $\mathcal Q$ will undo each other. Phrased mathematically,
	\[
	    \mathcal R\circ \mathcal Q=\Ident\qquad\text{and}\qquad \mathcal Q\circ \mathcal R=\Ident.
	\]
	Therefore, $\mathcal Q$ is an inverse of $\mathcal R$, and so $\mathcal R$ is invertible. 
\end{example}

One important fact about linear transformations is that if a linear transformation is invertible,
its inverse is also a linear transformation.
\begin{theorem}
	Let $\mathcal T$ be an invertible linear transformation. Then $\mathcal T^{-1}$ is also
	a linear transformation.
\end{theorem}
\begin{proof}
	Let $\mathcal T$ be an invertible linear transformation and let $\mathcal T^{-1}$ be its inverse.
	We need to show that (i) $\mathcal T^{-1}$ distributes over addition and (ii) $\mathcal T^{-1}$
	distributes over scalar multiplication.
	\begin{enumerate}
		\item[(i)] First observe that since $\mathcal T\circ \mathcal T^{-1}=\Ident$ and because $\mathcal T$ is linear, we have
		\[
			\vec a+\vec b = \mathcal T\circ \mathcal T^{-1}\vec a+\mathcal T\circ \mathcal T^{-1}\vec b=
			\mathcal T(\mathcal T^{-1}\vec a+ \mathcal T^{-1}\vec b).
		\]
		Since $\mathcal T^{-1}\circ \mathcal T=\Ident$, by using the fact that
			$\vec a+\vec b=\mathcal T(\mathcal T^{-1}\vec a+ \mathcal T^{-1}\vec b)$ we know
		\[
			\mathcal T^{-1}(\vec a+\vec b) = 
			\mathcal T^{-1}\Big(\mathcal T(\mathcal T^{-1}\vec a+ \mathcal T^{-1}\vec b)\Big)=
			\mathcal T^{-1}\vec a+ \mathcal T^{-1}\vec b.
		\]
		\item[(ii)] Similar to the proof of (i), we see
		\[
			\mathcal T^{-1}(\alpha\vec a) = \mathcal T^{-1}\Big(\alpha \big(\mathcal T\circ \mathcal T^{-1}\vec a\big)\Big)
			=\mathcal T^{-1}\circ \mathcal T(\alpha \mathcal T^{-1}\vec a) = \alpha\mathcal T^{-1}\vec a.
		\]
	\end{enumerate}
\end{proof}

\Heading{Invertibility and Matrices}

In the world of matrices, the \emph{identity matrix} takes the place of the identity function.

\SavedDefinitionRender{IdentityMatrix}

We can now define what it means for a matrix to be invertible\footnote{ This should look very similar to what it means
for a function to be invertible.}.

\SavedDefinitionRender{MatrixInverse}

\begin{example}
	Determine whether the matrices $A=\mat{2&5\\-3&-7}$ and $B=\mat{-7&-5\\3&2}$ are inverses of each other.

	\begin{align*}
	    AB &=\mat{2&5\\-3&-7}\mat{-7&-5\\3&2} =\mat{1&0\\0&1} = I\\
	    BA &=\mat{-7&-5\\3&2}\mat{2&5\\-3&-7} =\mat{1&0\\0&1} = I .
	\end{align*}

	Therefore,  $A$ and $B$ are inverses of each other.
\end{example}

\begin{example}
	Determine whether the matrices $A=\mat{2&5&0\\-3&-7&0}$ and $B=\mat{-7&-5\\3&2\\1&1}$ are inverses of each other.

	\[
	    AB =\mat{2&5&0\\-3&-7&0}\mat{-7&-5\\3&2\\1&1}=\mat{1&0\\0&1}=I
	    \]but\[
	    BA =\mat{-7&-5\\3&2\\1&1}\mat{2&5&0\\-3&-7&0}=\mat{1&0&0\\0&1&0\\-1&-2&0}\neq I.
	\]
	Therefore,  $A$ and $B$ are not inverses of each other.
\end{example}

Since every matrix induces a linear transformation, we can use the facts we know about invertible linear
transformations to produce facts about invertible matrices. In particular:
\begin{itemize}
	\item An $n\times m$ matrix $A$ is invertible if and only if $\Nullity(A)=0$ and $\Rank(A)=n$.
	\item An $n\times n$ matrix $A$ is invertible if and only if $\Nullity(A)=0$.
	\item An $n\times n$ matrix $A$ is invertible if and only if $\Rank(A)=n$.
\end{itemize}

\Heading{Matrix Algebra}

The linear equation $ax=b$ has solution $x=\frac{b}{a}$ whenever $a\neq 0$.  We arrive at this solution by
dividing both sides of the equation by $a$. Does a similar process exist for solving the matrix equation $A\vec x=\vec b$? It
sure does!

Unfortunately, we cannot divide by a matrix, but to solve $A\vec x=\vec b$, we don't need to ``divide'' by a matrix, we just need to
eliminate $A$ from the left side. This could be accomplished by using an inverse.

Suppose $A$ is invertible, then
\[
	A\vec x=\vec b\qquad\implies\qquad A^{-1}A\vec x=A^{-1}\vec b\qquad\implies\qquad \vec x=A^{-1}\vec b.
\]
Thus, if we have the inverse of a matrix handy, we can use it to solve a system of equations.

\begin{example}
	Use the fact that $\mat{2&5\\-3&-7}^{-1}=\mat{-7&-5\\3&2}$ to solve the system $\systeme{2x+5y=2,-3x-7y=1}$.

	The system can be rewritten as
	\[
	    \mat{2&5\\-3&-7}\mat{x\\y}=\mat{2\\1}.
	\]
	Multiplying both sides by $\mat{2&5\\-3&-7}^{-1}$ gives 
	\[
	    \mat{x\\y}=\mat{2&5\\-3&-7}^{-1}\mat{2\\1}=\mat{-7&-5\\3&2}\mat{2\\1}=\mat{-19\\8}.
	\]
\end{example}

It's important to note that, unlike in the case with regular scalars, the order of matrix multiplication matters.
So, whereas with scalars you could get away with something like
\[
	ax=b \qquad\implies \qquad \tfrac{1}{a}ax=b\tfrac{1}{a}\qquad\implies \qquad x=\tfrac{b}{a},
\]
with matrices $A\vec x=\vec b$ \emph{does not imply} $A^{-1}A\vec x=\vec b A^{-1}$. In fact, if $\vec b$ is a column
vector, the expression $\vec bA^{-1}$ is almost always undefined!

\Heading{Finding a Matrix Inverse}
Whereas before we only knew how to solve a matrix equation $A\vec x=\vec b$ using row reduction, we now
know how to use $A^{-1}$ to solve the same system. In fact, $A^{-1}$ is the exact matrix so that $\vec x=A^{-1}\vec b$
is the solution to $A\vec x=\vec b$. Therefore, by picking different $\vec b$'s and solving for $\vec x$, we
can find $A^{-1}$.

\begin{example}
	Let $A=\mat{2&5\\-3&-7}$. Find $A^{-1}$.

	We know $A^{-1}=\mat{a&b\\c&d}$ will be a $2\times 2$ matrix, and we know $\vec x=A^{-1}\vec b$ will always
	be the unique solution to $A\vec x=\vec b$. Therefore, we can find $A^{-1}$ by finding $\vec x,\vec b$ pairs that
	satisfy $A\vec x=\vec b$.

	Using row reduction, we see
	\[
		A\vec x=\mat{1\\0}\quad\text{has solution}\quad \vec x=\mat{-7\\3}, 
		\qquad\text{and}
		\qquad
		A\vec x=\mat{0\\1}\quad\text{has solution}\quad \vec x=\mat{-5\\2}.
	\]
	Therefore
	\[
		A^{-1}\mat{1\\0}=\mat{a\\c}=\mat{-7\\3}\qquad \text{and}\qquad
		A^{-1}\mat{0\\1}=\mat{b\\d}=\mat{-5\\2},
	\]
	and so
	\[
		A^{-1}=\mat{-7&-5\\3&2}.
	\]
\end{example}

\Heading{Elementary Matrices}

Finding the inverse of a matrix can be a lot of work. However
if you already know how to undo what the matrix does, finding the inverse might not be so hard. For example, 
if $R_{30}$ is the matrix that rotates vectors in $\R^2$ counter-clockwise by $30^\circ$, its inverse must be
$R_{-30}$, the matrix that rotates vectors in $\R^2$ \emph{clockwise} by $30^\circ$.

Like before when we analyzed linear transformations by breaking them up into compositions
of simpler linear transformations, another strategy for finding an inverse matrix is to break a matrix
into simpler ones whose inverses we can just write down.

Some of the simplest matrices around are the \emph{elementary matrices}.

\SavedDefinitionRender{ElementaryMatrix}

Examples of elementary matrices include
\[
	\mat{1&0&0\\0&1&0\\0&0&-5}\qquad
	\mat{1&0&7\\0&1&0\\0&0&1}\qquad
	\mat{0&1&0\\1&0&0\\0&0&1}.
\]
These matrices are obtained from the row operations ``multiply the last row by $-5$'', ``add $7$ times the last row to the first'',
and ``swap the first two rows''.

Elementary matrices are useful because multiplying by an elementary matrix performs the corresponding elementary row operation! See for
yourself:
\begin{align*}
	\mat{1&0&0\\0&1&0\\0&0&-5}
	\mat{a&b&c\\d&e&f\\g&h&i}
	&=
	\mat{a&b&c\\d&e&f\\-5g&-5h&-5i}
\\
	\mat{1&0&7\\0&1&0\\0&0&1}
	\mat{a&b&c\\d&e&f\\g&h&i}
	&=
	\matc{a+7g&b+7h&c+7i\\d&e&f\\g&h&i}
\\
	\mat{0&1&0\\1&0&0\\0&0&1}
	\mat{a&b&c\\d&e&f\\g&h&i}
	&=
	\mat{d&e&f\\a&b&c\\g&h&i}
\end{align*}
As a refresher, the elementary row operations are:
\begin{itemize}
	\item multiply a row by a non-zero constant;
	\item add a multiple of one row to another; and
	\item swap two rows.
\end{itemize}
Each one of these operations can be undone, and so every elementary matrix is invertible. What's more, the inverse is
another elementary matrix that is
easy to write down.

\begin{example}
	Find the inverse of $E=
	\mat{1&0&7\\0&1&0\\0&0&1}$.

	Since $E$ corresponds to the row operation ``add $7$ times the last row to the first'', $E^{-1}$ must correspond
	to the row operation ``subtract $7$ times the last row from the first''. Therefore,
	\[
		E^{-1}=\mat{1&0&-7\\0&1&0\\0&0&1}.
	\]
\end{example}

\Heading{Elementary Matrices and Inverses}

For a matrix $M$ to be invertible, we know that $M$ must be square and $\Nullity(M)=0$. That means,
$M$ is invertible if and only if $\Rref(M)=I$. In other words, $M$ is invertible if there is a sequence of elementary row operations
that turn $M$ into $I$. Each one of these row operations can be represented by an elementary matrix, which gives us the
following theorem.

\begin{theorem}
	A matrix $M$ is invertible\index{Matrix!invertible} if and only if there are elementary matrices $E_1,\ldots, E_k$ so that
	\[
		E_k\cdots E_2E_1M=I.
	\]
\end{theorem}

Now, suppose $M$ is invertible and let $E_1,\ldots,E_k$ be elementary matrices so that $E_k\cdots E_2E_1M=I$. We now know
\[
	E_k\cdots E_2E_1M=\underbrace{(E_k\cdots E_2E_1)}_QM=QM=I.
\]
If we can argue that $MQ=I$, then $Q$ will be the inverse of $M$!

\begin{theorem}
	If $A$ is a square matrix and $AB=I$ for some matrix $B$, then
	$BA=I$.
\end{theorem}
\begin{proof}
	Suppose $A$ is a square matrix and that $AB=I$. Since $AB=I$, $B$ must also be square.
	Since $\Null(B)\subseteq\Null(AB)$, we know $\Nullity(B)\leq \Nullity(AB)=\Nullity(I)=0$, 
	and so $B$ is invertible (since it's a square matrix whose nullity is $0$). Let $B^{-1}$
	be the inverse of $B$. Observe now that
	\[
		A=AI=A(BB^{-1})=(AB)B^{-1}=IB^{-1}=B^{-1},
	\]
	and so $A=B^{-1}$. Finally, substituting $B^{-1}$ for $A$ shows
	\[
		BA=BB^{-1}=I.
	\]
\end{proof}

In light of this theorem, we now have a new algorithm for finding the inverse of a matrix---find
elementary matrices that turn the matrix into the identity matrix and multiply those elementary
matrices together to find the inverse.

\begin{example}
	Let $A=\mat{1&2&0\\0&4&0\\0&-1&1}$. Find $A^{-1}$ using elementary matrices.

	We can row-reduce $A$ with the following steps:
	\[
	    \mat{1&2&0\\0&4&0\\0&-1&1}\to \mat{1&2&0\\0&1&0\\0&-1&1}\to \mat{1&0&0\\0&1&0\\0&-1&1}\to \mat{1&0&0\\0&1&0\\0&0&1} 
	\]
	The elementary matrices corresponding to these steps are
	\[
	    E_1=\mat{1&0&0\\0&\frac{1}{4}&0\\0&0&1} \qquad E_2=\mat{1&-2&0\\0&1&0\\0&0&1}\qquad E_3=\mat{1&0&0\\0&1&0\\0&1&1}.
	\]
	We now have 
	\[
	    E_3 E_2 E_1 A = I,
	\]
	and so
	\[
	    A^{-1}=E_3 E_2 E_1 = \mat{1&0&0\\0&1&0\\0&1&1}\mat{1&-2&0\\0&1&0\\0&0&1}\mat{1&0&0\\0&\frac{1}{4}&0\\0&0&1} = \mat{1&-\frac{1}{2}&0\\ 0&\frac{1}{4}&0\\ 0&\frac{1}{4}&1}.
	\]
\end{example}

\Heading{Decomposition into Elementary Matrices}

If $A$ is an invertible matrix, then the double-inverse of $A$ (i.e., $(A^{-1})^{-1}$) is $A$ itself\footnote{ Formally
we say that the operation of taking a matrix inverse is an \emph{involution}.}. This is easily proved. By definition, $(A^{-1})^{-1}$
is a matrix $B$ so that $BA^{-1}=I$ and $A^{-1}B=I$. But $B=A$ satisfies this condition! 

Now, suppose $M$ is an invertible matrix. Then, there exists a sequence of elementary matrices $E_1,\ldots, E_k$ so that
$E_k\cdots E_2E_1M=I$ and 
\[
	M^{-1}=E_k\cdots E_2E_1.
\]
Therefore 
\[
	M=(M^{-1})^{-1} = (E_k\cdots E_2E_1)^{-1}.
\]
Thinking carefully about what $(E_k\cdots E_2E_1)^{-1}$ should be, we see that 
\[
	(E_1^{-1}E_2^{-1}\cdots E_k^{-1})E_k\cdots E_2E_1=I\qquad\text{and}\qquad E_k\cdots E_2E_1(E_1^{-1}E_2^{-1}\cdots E_k^{-1})=I,
\]
and so
\[
	M=(E_k\cdots E_2E_1)^{-1}=E_1^{-1}E_2^{-1}\cdots E_k^{-1}.
\]
(Notice the order of matrix multiplication reversed!)
Each $E_i^{-1}$ is also an elementary matrix, and so we have just shown that every invertible matrix can
be written as the product of elementary matrices. This is actually a double-sided implication (if and only if).

\begin{theorem}
	A matrix $M$ is invertible if and only if it can be written as the product of elementary matrices.
\end{theorem}
\begin{proof}
	Suppose $M$ is invertible. Then, there exists a sequence of elementary matrices $E_1,\ldots,E_k$ so that
	$E_k\cdots E_1M=I$. It follows that
	\[
		M=E_1^{-1}E_2^{-1}\cdots E_k^{-1}
	\]
	is the product of elementary matrices. Conversely, since the product of invertible matrices is invertible
	and every elementary matrix is invertible, the product of elementary matrices must be invertible.
	Therefore, if $M$ is not invertible, it \emph{cannot} be written as the product of elementary matrices.
\end{proof}
